@inproceedings{Long2019,
author = {Long, Duri and Jacob, Mikhail and Magerko, Brian},
title = {Designing Co-Creative AI for Public Spaces},
year = {2019},
isbn = {9781450359177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325480.3325504},
doi = {10.1145/3325480.3325504},
abstract = {Artificial intelligence (AI) is becoming increasingly pervasive in our everyday lives. There are consequently many common misconceptions about what AI is, what it is capable of, and how it works. Compounding the issue, opportunities to learn about AI are often limited to audiences who already have access to and knowledge about technology. Increasing access to AI in public spaces has the potential to broaden public AI literacy, and experiences involving co-creative (i.e. collaboratively creative) AI are particularly well-suited for engaging a broad range of participants. This paper explores how to design co-creative AI for public interaction spaces, drawing both on existing literature and our own experiences designing co-creative AI for public venues. It presents a set of design principles that can aid others in the development of co-creative AI for public spaces as well as guide future research agendas.},
booktitle = {Proceedings of the 2019 Conference on Creativity and Cognition},
pages = {271–284},
numpages = {14},
keywords = {reflection on design processes, public displays, human-centered ai, collaboration, co-creative ai},
location = {<conf-loc>, <city>San Diego</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {C&C '19}
}

@inproceedings{Wallace2023,
author = {Wallace, Benedikte and Hilton, Clarice and Nymoen, Kristian and Torresen, Jim and Martin, Charles Patrick and Fiebrink, Rebecca},
title = {Embodying an Interactive AI for Dance Through Movement Ideation},
year = {2023},
isbn = {9798400701801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591196.3593336},
doi = {10.1145/3591196.3593336},
abstract = {What expectations exist in the minds of dancers when interacting with a generative machine learning model? During two workshop events, experienced dancers explore these expectations through improvisation and role-play, embodying an imagined AI-dancer. The dancers explored how intuited flow, shared images, and the concept of a human replica might work in their imagined AI-human interaction. Our findings challenge existing assumptions about what is desired from generative models of dance, such as expectations of realism, and how such systems should be evaluated. We further advocate that such models should celebrate non-human artefacts, focus on the potential for serendipitous moments of discovery, and that dance practitioners should be included in their development. Our concrete suggestions show how our findings can be adapted into the development of improved generative and interactive machine learning models for dancers’ creative practice.},
booktitle = {Proceedings of the 15th Conference on Creativity and Cognition},
pages = {454–464},
numpages = {11},
keywords = {dance, embodiment, generative AI, reflexive thematic analysis},
location = {, Virtual Event, USA, },
series = {C&C '23}
}

@inproceedings{Wallace2021,
author = {Wallace, Benedikte and Martin, Charles P. and T\o{}rresen, Jim and Nymoen, Kristian},
title = {Learning Embodied Sound-Motion Mappings: Evaluating AI-Generated Dance Improvisation},
year = {2021},
isbn = {9781450383769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450741.3465245},
doi = {10.1145/3450741.3465245},
abstract = {Through dance, a wide range of emotions can be expressed. As virtual agents and robots continue to become part of our daily lives, the need for them to efficiently convey emotion and intent increases. When trained to dance, to what extent can AI learn to model the tacit mappings between sound and motion? Here, we explore the creative capacity of a generative model trained on 3D motion capture recordings of improvised dance. We perform a perceptual judgment experiment wherein respondents rate movement generated by our model as well as human performances. While the sound-motion mappings remain somewhat elusive, particularly when compared to examples of human dance, our study shows that in certain aspects related to perceived dance-likeness and expressivity, the model successfully mimics human dance movement. By employing a perceptual study to evaluate our generative model, we aim to further our ability to understand the affordances and limitations of creative AI.},
booktitle = {Proceedings of the 13th Conference on Creativity and Cognition},
articleno = {43},
numpages = {9},
keywords = {Perceptual judgement experiment, Mixture Density Networks, Generative AI, Embodied Music Cognition, Dance},
location = {Virtual Event, Italy},
series = {C&C '21}
}

@article{Wallace2022,
author = {Wallace, Benedikte and Martin, Charles},
title = {Embodying the Glitch: Perspectives on Generative AI in Dance Practice}
year = {2022},
month = {10},
url = {https://doi.org/10.48550/arXiv.2210.09291},
doi = {10.48550/arXiv.2210.09291},
}

@article{Francoise2018,
author = {Fran\c{c}oise, Jules and Bevilacqua, Fr\'{e}d\'{e}ric},
title = {Motion-Sound Mapping through Interaction: An Approach to User-Centered Design of Auditory Feedback Using Machine Learning},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3211826},
doi = {10.1145/3211826},
abstract = {Technologies for sensing movement are expanding toward everyday use in virtual reality, gaming, and artistic practices. In this context, there is a need for methodologies to help designers and users create meaningful movement experiences. This article discusses a user-centered approach for the design of interactive auditory feedback using interactive machine learning. We discuss Mapping through Interaction, a method for crafting sonic interactions from corporeal demonstrations of embodied associations between motion and sound. It uses an interactive machine learning approach to build the mapping from user demonstrations, emphasizing an iterative design process that integrates acted and interactive experiences of the relationships between movement and sound. We examine Gaussian Mixture Regression and Hidden Markov Regression for continuous movement recognition and real-time sound parameter generation. We illustrate and evaluate this approach through an application in which novice users can create interactive sound feedback based on coproduced gestures and vocalizations. Results indicate that Gaussian Mixture Regression and Hidden Markov Regression can efficiently learn complex motion-sound mappings from few examples.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = {jun},
articleno = {16},
numpages = {30},
keywords = {user-centered design, sound and music computing, sonification, programming-by-demonstration, movement, Interactive machine learning}
}

// URL: https://computationalcreativity.net/iccc21/wp-content/uploads/2021/09/ICCC_2021_paper_114.pdf
@inproceedings{Ibrahimi2021
author = {Sarah Ibrahimi},
title = {Composition, Performance and Evaluation: A Dance Education Framework for AI Systems},
year = {2021},
booktitle = {Proceedings of the 12th International Conference on Computational Creativity},
isbn = {9789895416035},
}

@article{Pettee2019,
title={Beyond Imitation: Generative and Variational Choreography via Machine Learning}, 
author={Mariel Pettee and Chase Shimmin and Douglas Duhaime and Ilya Vidrin},
year={2019},
eprint={1907.05297},
archivePrefix={arXiv},
primaryClass={cs.LG}
}

@inproceedings{10.1145/3563703.3596658,
author = {Trajkova, Milka and Deshpande, Manoj and Knowlton, Andrea and Monden, Cassandra and Long, Duri and Magerko, Brian},
title = {AI Meets Holographic Pepper’s Ghost: A Co-Creative Public Dance Experience},
year = {2023},
isbn = {9781450398985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563703.3596658},
doi = {10.1145/3563703.3596658},
abstract = {In this demonstration, we present a holographic projected version of LuminAI, which is an interactive art installation that allows participants to collaborate with an AI dance partner by improvising movements together. By utilizing a mix of a top-down and bottom-up approach, we seek to understand embodied co-creativity in an improvisational dance setting to better develop the design of the modular AI agent to creatively collaborate with a dancer. The purpose of this demonstration is to describe the five-module agent design and investigate how we can design an immersive experience that is design-efficient, portable, light, and duo-user participation. Through this installation in an imitated black box space, audience members and dancers engage in an immersive co-creative dance experience, inspiring discussion on the limitless applications of dance and technology in the realms of learning, training, and creativity.},
booktitle = {Companion Publication of the 2023 ACM Designing Interactive Systems Conference},
pages = {274–278},
numpages = {5},
keywords = {dance improvisation, co-creativity, co-creative agents, co-creative AI, AI agents},
location = {Pittsburgh, PA, USA},
series = {DIS '23 Companion}
}

@article{RuilongLi202,
title={AI Choreographer: Music Conditioned 3D Dance Generation with AIST++}, 
author={Ruilong Li and Shan Yang and David A. Ross and Angjoo Kanazawa},
year={2021},
eprint={2101.08779},
archivePrefix={arXiv},
primaryClass={cs.CV}
}

@inproceedings{Huang2022,
author={Huang, Yuhang and Zhang, Junjie and Liu, Shuyan and Bao, Qian and Zeng, Dan and Chen, Zhineng and Liu, Wu},
booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={Genre-Conditioned Long-Term 3D Dance Generation Driven by Music}, 
year={2022},
volume={},
number={},
pages={4858-4862},
keywords={Solid modeling;Three-dimensional displays;Conferences;Music;Signal processing;Transformers;Decoding;3D dance generation;genre-conditioned;modality fusion;music-driven},
doi={10.1109/ICASSP43922.2022.9747838}
}

@inproceedings{Kim2022,
author={Kim, Jinwoo and Oh, Heeseok and Kim, Seongjean and Tong, Hoseok and Lee, Sanghoon},
booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={A Brand New Dance Partner: Music-Conditioned Pluralistic Dancing Controlled by Multiple Dance Genres}, 
year={2022},
volume={},
number={},
pages={3480-3490},
keywords={Training;Measurement;Humanities;Computer vision;Computational modeling;Computer architecture;Transformers;Image and video synthesis and generation; Action and event recognition; Deep learning architectures and techniques; Vision + X},
doi={10.1109/CVPR52688.2022.00348}
}

@article{SiyaoLi2023,
author={Siyao, Li and Yu, Weijiang and Gu, Tianpei and Lin, Chunze and Wang, Quan and Qian, Chen and Loy, Chen Change and Liu, Ziwei},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Bailando++: 3D Dance GPT With Choreographic Memory}, 
year={2023},
volume={45},
number={12},
pages={14192-14207},
keywords={Humanities;Three-dimensional displays;Codes;Transformers;Avatars;Rhythm;Encoding;3D human motion;dance generation;GPT;multi-modal;VQ-VAE},
doi={10.1109/TPAMI.2023.3319435}
}

@inproceedings{10.1145/3503161.3548090,
author = {Wang, Zixuan and Jia, Jia and Wu, Haozhe and Xing, Junliang and Cai, Jinghe and Meng, Fanbo and Chen, Guowen and Wang, Yanfeng},
title = {GroupDancer: Music to Multi-People Dance Synthesis with Style Collaboration},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548090},
doi = {10.1145/3503161.3548090},
abstract = {Different people dance in different styles. So when multiple people dance together, the phenomenon of style collaboration occurs: people need to seek common points while reserving differences in various dancing periods. Thus, we introduce a novel Music-driven Group Dance Synthesis task. Compared with single-people dance synthesis explored by most previous works, modeling the style collaboration phenomenon and choreographing for multiple people are more complicated and challenging. Moreover, the lack of sufficient records for conducting multi-people choreography in prior datasets further aggravates this problem. To address these issues, we construct a rich-annotated 3D Multi-Dancer Choreography dataset (MDC) and newly devise a metric SCEU for style collaboration evaluation. To our best knowledge, MDC is the first 3D dance dataset that collects both individual and collaborated music-dance pairs. Based on MDC, we present a novel framework, GroupDancer, consisting of three stages: Dancer Collaboration, Motion Choreography and Motion Transition. The Dancer Collaboration stage determines when and which dancers should collaborate their dancing styles from music. Afterward, the Motion Choreography stage produces a motion sequence for each dancer. Finally, the Motion Transition stage fills the gaps between the motions to achieve fluent and natural group dance. To make GroupDancer trainable from end to end and able to synthesize group dance with style collaboration, we propose mixed training and selective updating strategies. Comprehensive evaluations on the MDC dataset demonstrate that the proposed GroupDancer model can synthesize quite satisfactory group dance synthesis results with style collaboration.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {1138–1146},
numpages = {9},
keywords = {style collaboration, group dance synthesis, choreography},
location = {<conf-loc>, <city>Lisboa</city>, <country>Portugal</country>, </conf-loc>},
series = {MM '22}
}

@article{10.1145/3618356,
author = {Le, Nhat and Do, Tuong and Do, Khoa and Nguyen, Hien and Tjiputra, Erman and Tran, Quang D. and Nguyen, Anh},
title = {Controllable Group Choreography Using Contrastive Diffusion},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3618356},
doi = {10.1145/3618356},
abstract = {Music-driven group choreography poses a considerable challenge but holds significant potential for a wide range of industrial applications. The ability to generate synchronized and visually appealing group dance motions that are aligned with music opens up opportunities in many fields such as entertainment, advertising, and virtual performances. However, most of the recent works are not able to generate high-fidelity long-term motions, or fail to enable controllable experience. In this work, we aim to address the demand for high-quality and customizable group dance generation by effectively governing the consistency and diversity of group choreographies. In particular, we utilize a diffusion-based generative approach to enable the synthesis of flexible number of dancers and long-term group dances, while ensuring coherence to the input music. Ultimately, we introduce a Group Contrastive Diffusion (GCD) strategy to enhance the connection between dancers and their group, presenting the ability to control the consistency or diversity level of the synthesized group animation via the classifier-guidance sampling technique. Through intensive experiments and evaluation, we demonstrate the effectiveness of our approach in producing visually captivating and consistent group dance motions. The experimental results show the capability of our method to achieve the desired levels of consistency and diversity, while maintaining the overall quality of the generated group choreography.},
journal = {ACM Trans. Graph.},
month = {dec},
articleno = {224},
numpages = {14},
keywords = {diffusion models, group choreography animation, group motion synthesis, machine learning}
}

@article{Yao2023,
title={Dance with You: The Diversity Controllable Dancer Generation via Diffusion Models}, 
author={Siyue Yao and Mingjie Sun and Bingliang Li and Fengyu Yang and Junle Wang and Ruimao Zhang},
year={2023},
eprint={2308.13551},
archivePrefix={arXiv},
primaryClass={cs.HC}
}

@inproceedings{10.1145/3581783.3612307,
author = {Qi, Qiaosong and Zhuo, Le and Zhang, Aixi and Liao, Yue and Fang, Fei and Liu, Si and Yan, Shuicheng},
title = {DiffDance: Cascaded Human Motion Diffusion Model for Dance Generation},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612307},
doi = {10.1145/3581783.3612307},
abstract = {When hearing music, it is natural for people to dance to its rhythm. Automatic dance generation, however, is a challenging task due to the physical constraints of human motion and rhythmic alignment with target music. Conventional autoregressive methods introduce compounding errors during sampling and struggle to capture the long-term structure of dance sequences. To address these limitations, we present a novel cascaded motion diffusion model, DiffDance, designed for high-resolution, long-form dance generation. This model comprises a music-to-dance diffusion model and a sequence super-resolution diffusion model. To bridge the gap between music and motion for conditional generation, DiffDance employs a pretrained audio representation learning model to extract music embeddings and further align its embedding space to motion via contrastive loss. During training our cascaded diffusion model, we also incorporate multiple geometric losses to constrain the model outputs to be physically plausible and add a dynamic loss weight that adaptively changes over diffusion timesteps to facilitate sample diversity. Through comprehensive experiments performed on the benchmark dataset AIST++, we demonstrate that DiffDance is capable of generating realistic dance sequences that align effectively with the input music. These results are comparable to those achieved by state-of-the-art autoregressive methods.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {1374–1382},
numpages = {9},
keywords = {conditional generation, diffusion model, multimodal learning, music-to-dance},
location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
series = {MM '23}
}

@inproceedings{Bisig2022,
author = {Bisig, Daniel and Wegner, Ephraim},
title = {Puppeteering AI -Interactive Control of an Artificial Dancer},
year = {2022},
month = {05},
pages = {},
}

@article{Lee2019,
title={Dancing to Music}, 
author={Hsin-Ying Lee and Xiaodong Yang and Ming-Yu Liu and Ting-Chun Wang and Yu-Ding Lu and Ming-Hsuan Yang and Jan Kautz},
year={2019},
eprint={1911.02001},
archivePrefix={arXiv},
primaryClass={cs.CV}
}

@article{JiamanLi2020,
title={Learning to Generate Diverse Dance Motions with Transformer}, 
author={Jiaman Li and Yihang Yin and Hang Chu and Yi Zhou and Tingwu Wang and Sanja Fidler and Hao Li},
year={2020},
eprint={2008.08171},
archivePrefix={arXiv},
primaryClass={cs.CV}
}

@misc{BuyuLi2023,
title={DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer}, 
author={Buyu Li and Yongchi Zhao and Zhelun Shi and Lu Sheng},
year={2023},
eprint={2103.10206},
archivePrefix={arXiv},
primaryClass={cs.AI}
}

@misc{Ahn2019,
title={Generative Autoregressive Networks for 3D Dancing Move Synthesis from Music}, 
author={Hyemin Ahn and Jaehun Kim and Kihyun Kim and Songhwai Oh},
year={2019},
eprint={1911.04069},
archivePrefix={arXiv},
primaryClass={cs.LG}
}

@inproceedings{Tang2018,
author = {Tang, Taoran and Jia, Jia and Mao, Hanyang},
title = {Dance with Melody: An LSTM-autoencoder Approach to Music-oriented Dance Synthesis},
year = {2018},
isbn = {9781450356657},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240508.3240526},
doi = {10.1145/3240508.3240526},
abstract = {Dance is greatly influenced by music. Studies on how to synthesize music-oriented dance choreography can promote research in many fields, such as dance teaching and human behavior research. Although considerable effort has been directed toward investigating the relationship between music and dance, the synthesis of appropriate dance choreography based on music remains an open problem. There are two main challenges: 1) how to choose appropriate dance figures, i.e., groups of steps that are named and specified in technical dance manuals, in accordance with music and 2) how to artistically enhance choreography in accordance with music. To solve these problems, in this paper, we propose a music-oriented dance choreography synthesis method using a long short-term memory (LSTM)-autoencoder model to extract a mapping between acoustic and motion features. Moreover, we improve our model with temporal indexes and a masking method to achieve better performance. Because of the lack of data available for model training, we constructed a music-dance dataset containing choreographies for four types of dance, totaling 907,200 frames of 3D dance motions and accompanying music, and extracted multidimensional features for model training. We employed this dataset to train and optimize the proposed models and conducted several qualitative and quantitative experiments to select the best-fitted model. Finally, our model proved to be effective and efficient in synthesizing valid choreographies that are also capable of musical expression.},
booktitle = {Proceedings of the 26th ACM International Conference on Multimedia},
pages = {1598–1606},
numpages = {9},
keywords = {music-dance dataset, motion synthesis, lstm, autoencoder, 3d motion capture},
location = {Seoul, Republic of Korea},
series = {MM '18}
}

@article{Holden2016,
author = {Holden, Daniel and Saito, Jun and Komura, Taku},
title = {A deep learning framework for character motion synthesis and editing},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2897824.2925975},
doi = {10.1145/2897824.2925975},
abstract = {We present a framework to synthesize character movements based on high level parameters, such that the produced movements respect the manifold of human motion, trained on a large motion capture dataset. The learned motion manifold, which is represented by the hidden units of a convolutional autoencoder, represents motion data in sparse components which can be combined to produce a wide range of complex movements. To map from high level parameters to the motion manifold, we stack a deep feedforward neural network on top of the trained autoencoder. This network is trained to produce realistic motion sequences from parameters such as a curve over the terrain that the character should follow, or a target location for punching and kicking. The feedforward control network and the motion manifold are trained independently, allowing the user to easily switch between feedforward networks according to the desired interface, without re-training the motion manifold. Once motion is generated it can be edited by performing optimization in the space of the motion manifold. This allows for imposing kinematic constraints, or transforming the style of the motion, while ensuring the edited motion remains natural. As a result, the system can produce smooth, high quality motion sequences without any manual pre-processing of the training data.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {138},
numpages = {11},
keywords = {autoencoder, character animation, convolutional neural networks, deep learning, human motion, manifold learning}
}

@inproceedings{Alemi2017,
author = {Alemi, Omid and Françoise, Jules and Pasquier, Philippe},
title = {GrooveNet: Real-Time Music-Driven Dance Movement Generation using Artificial Neural Networks}
year = {2017},
month = {08},
pages = {},
}

@article{Sun2021,
author={Sun, Guofei and Wong, Yongkang and Cheng, Zhiyong and Kankanhalli, Mohan S. and Geng, Weidong and Li, Xiangdong},
journal={IEEE Transactions on Multimedia}, 
title={DeepDance: Music-to-Dance Motion Choreography With Adversarial Learning}, 
year={2021},
volume={23},
number={},
pages={497-509},
keywords={Generators;Feature extraction;Task analysis;Correlation;Three-dimensional displays;Music;Deep learning;Music-driven dance choreography;adversarial learning;cross-modal association},
doi={10.1109/TMM.2020.2981989}
}
