@inproceedings{Long2019,
   abstract = {Artificial intelligence (AI) is becoming increasingly pervasive in our everyday lives. There are consequently many common misconceptions about what AI is, what it is capable of, and how it wor$
   author = {Duri Long and Mikhail Jacob and Brian Magerko},
   doi = {10.1145/3325480.3325504},
   isbn = {9781450359177},
   journal = {C and C 2019 - Proceedings of the 2019 Creativity and Cognition},
   keywords = {Co-creative ai,Collaboration,Human-centered ai,Public displays,Reflection on design processes},
   month = {6},
   note = {<b>What is the interaction framework for public AI, learnings</b>:<br/>1. multiple ways of interacting with it. (capable for more advanced users as well as children for example)<br/>2. should faci$
   pages = {271-284},
   publisher = {Association for Computing Machinery, Inc},
   title = {Designing co-creative AI for public spaces},
   year = {2019},
}
@book{Kantosalo2020,
   abstract = {E-zbornik. Nasl. z nasl. zaslona. Opis vira z dne 10. 9. 2020.},
   author = {Anna Kantosalo and Prashanth Thattai Ravikumar and Kazjon Grace and Tapio Takala},
   isbn = {9789895416028},
   publisher = {ACC = Association for Computational Creativity},
   title = {Modalities, Styles and Strategies: An Interaction Framework for
Human–Computer Co-Creativity},
   year = {2020},
}
@article{Deshpande2023,
   abstract = {This paper introduces a new method for quantifying open-ended collaborative embodied improvisation: Observable Creative Sense-Making (OCSM). This technique builds on previous work on Creative Sense-Making (CSM), examines its shortcomings, and addresses it by reformalizing and grounding CSM in current literature from embodied social cognition and an intersubjective perspective of creativity. We apply this method to empirical studies of human collaboration in dance improvisation with 16 advanced college dancers and establish the method's validity. The OCSM method described in this paper includes a qualitative coding technique, a web-based tool for coding the interaction, and the cognitive theory behind its application. CCS CONCEPTS • Human-centered computing → User studies; HCI design and evaluation methods.},
   author = {Manoj Deshpande and Milka Trajkova and Andrea Knowlton and Brian Magerko},
   doi = {10.1145/3591196.3593514},
   isbn = {9798400701801},
   keywords = {co-creation,embodiment,improvisation,social cognition},
   note = {<b>Main concept: (OCSM -> Obsevatory Creative Sense-Making)</b><br/>OCSM is a quantitative way to, in near real-time, for a computer or AI to gather information about the current on going interaction.<br/><br/><b>How does OCSM work</b>:<br/>Uses three curves to represent the state: Participation, Newness and Appropriateness. Each with a scale from 0-5. A coder will code a piece of dance after <br/><br/><b>Problem that it tried to solve</b>:<br/>Current quantification framework for interaction is too subjective and not observable. Want to develop a easy to use and also minimal inter-subjectivity framework.<br/><br/><b>How well it did that</b>:<br/>Not fine grained but the overall trend is observable. Resolution is around a minute (every minute's average state is captured well). This will likely not be good enough for a reward system for training models. but can be used to code data or analyze an improvisational session.<br/><br/><b>Future work</b>:<br/>Although movement is present, the intention is hidden. Still influenced by subjectivity, which means a more <br/><br/><b>Theories:</b><br/><i><u>Simulation Theory</u>:</i> At any time, an individual can use their own mind to model and understand another person's mental state. (empathy)<br/><i><u>Theory of Mind Theory</u>:</i> At any time, an individual can use general theories of how the mind works to figure out what other people are thinking (Psychology)<br/><br/><i><u>SM and TMT localizes cognition</u>:</i><br/>Both SM and TMT does not consider the embodied nature of interaction and treats cognition as a purely obsevational party instead of a actively interacting party.<br/><br/><i><u>Embodied Cognition</u></i>:<br/>cognition ariese from interactions with its surroundings. Information is not gathered purely passively but highly actively. There are no differences from mental and non-mental processes, and everything is part of the concious? Radical enavtivism refers to everything is in the world and there is no inner-representation or state.},
   title = {Observable Creative Sense-Making (OCSM): A Method For Quantifying Improvisational Co-Creative Interaction},
   url = {https://doi.org/10.1145/3591196.3593514},
   year = {2023},
}
@article{Jacy2024,
   abstract = {Robot autonomy is an influential and ubiquitous factor in human-robot interaction (HRI), but it is rarely discussed beyond a one-dimensional measure of the degree to which a robot operates without human intervention. As robots become more sophisticated, this simple view of autonomy could be expanded to capture the variety of autonomous behaviors robots can exhibit and to match the rich literature on human autonomy in philosophy, psychology, and other fields. In this paper, we conduct a systematic literature review of robot autonomy in HRI and integrate this with the broader literature into a taxonomy of six distinct forms of autonomy: those based on robot and human involvement at runtime (operational autonomy, intentional autonomy, shared autonomy), human involvement before runtime (non-deterministic autonomy), and expressions of autonomy at runtime (cognitive autonomy, physical autonomy). We discuss future considerations for autonomy in HRI that emerge from this study, including moral consequences, the idealization of "full" robot autonomy, and connections to agency and free will. CCS CONCEPTS • Human-centered computing → HCI theory, concepts and models; • General and reference → Surveys and overviews.},
   author = {Stephanie Kim Jacy and Reese Anthis and Sarah Sebo and Stephanie Kim},
   doi = {10.1145/3610977.3634993},
   isbn = {9798400703225},
   keywords = {Human-Robot Interaction,Literature Review,Philosophy,Robot Autonomy,Robotics,Taxonomy},
   publisher = {ACM},
   title = {A Taxonomy of Robot Autonomy for Human-Robot Interaction},
   volume = {13},
   url = {https://doi.org/10.1145/3610977.3634993},
   year = {2024},
}
@article{Rezwana2023,
   abstract = {Human-AI co-creativity involves both humans and AI collaborating on a shared creative product as partners. In a creative collaboration, interaction dynamics, such as turn-taking, contribution type, and communication, are the driving forces of the co-creative process. Therefore the interaction model is a critical and essential component for effective co-creative systems. There is relatively little research about interaction design in the co-creativity field, which is reflected in a lack of focus on interaction design in many existing co-creative systems. The primary focus of co-creativity research has been on the abilities of the AI. This article focuses on the importance of interaction design in co-creative systems with the development of the Co-Creative Framework for Interaction design (COFI) that describes the broad scope of possibilities for interaction design in co-creative systems. Researchers can use COFI for modeling interaction in co-creative systems by exploring alternatives in this design space of interaction. COFI can also be beneficial while investigating and interpreting the interaction design of existing co-creative systems. We coded a dataset of existing 92 co-creative systems using COFI and analyzed the data to show how COFI provides a basis to categorize the interaction models of existing co-creative systems. We identify opportunities to shift the focus of interaction models in co-creativity to enable more communication between the user and AI leading to human-AI partnerships.},
   author = {Jeba Rezwana and Mary Lou Maher},
   doi = {10.1145/3519026},
   journal = {ACM Trans. Comput.-Hum. Interact},
   keywords = {Framework,Human-AI co-creativity,Interaction design},
   note = {<b>The importance of interaction design</b><br/>Current research for co-creative systems focus on AI <br/>performance but lack good interaction deisgn:<br/>- kantosalo et al: interaction design should be ground zero for designing co-creative systems.<br/>- many co-creative systems only have a one-way communication, which is not enough for a co-creative process, lacking feedback, suggestions etc.<br/>- Study shows that users have a more engaging interction experience with AI when its text based instructions rather than just a button press.<br/><br/><b>Importance of creative collaboration:</b><br/>Sawyer argued that collaborative creativity is more creative than individual creativity. <br/><br/><b>Concepts and defs:</b><br/>Three aspects of interactions:<br/>- Interaction modalities,<br/>- interaction style,<br/>- interaction strategies.<br/><i>Parcipatory sense making</i>: when all creative parties are adapting their responses according to others' contributions, while keeping the interaction engaging.<br/><br/><b>Process of co-creativity:</b><br/>- turn-taking<br/>- timing of initative,<br/>- communication<br/><br/><br/><b><u>COFI:</u></b><br/><b>Interaction sense making process include interaction with other creatives, and interaction with the product. (adopted by COFI)</b><br/><i>Framework => \{</i><br/>- with other creatives: \{ (many CSCW stress the important of both the collaboration part and the communication part)<br/>   Collaboration style:<br/>   Communication style:<br/> \}<br/>- with the product: \{ (align with creativity literature which focuses on the process and the end product)<br/>   Creative process:<br/>   Creative product:<br/> \}<br/>\}},
   title = {Designing Creative AI Partners with COFI: A Framework for Modeling Interaction in Human-AI Co-Creative Systems; Designing Creative AI Partners with COFI: A Framework for Modeling Interaction in Human-AI Co-Creative Systems},
   volume = {30},
   url = {https://doi.org/10.1145/3519026},
   year = {2023},
}
@article{Yasunaga2012,
   abstract = {Motion-capture-based character animations are widely used in computer graphics and interactive games.In this paper, we show a novel approach to create dancing character an-imations that react to input music and an accelerometer manipulated by a user. Since the sensor reads express intensities of users' body movements, the system can synthesize character motions whose intensities are synchronized to those of users. Our system consists of analysis phase and synthesis phase. In the analysis phase, the musical beat and segments are detected from input sound, and motion rhythm and intensities are found from motion capture data. With the results of this analysis, we generate a motion graph that can generate character motions matched to the musical rhythm. In synthesis phase, the system receives the output data from an accelerometer and traverses the motion graph according to the matching result between the sensor data and the motion intensity. As the result, our system adds an interactive component to live dancing performed by virtual characters.},
   author = {Takuya Yasunaga and Atsushi Nakazawa and Haruo Takemura},
   isbn = {9781450310895},
   keywords = {I37 [Computer Graphics]: Three-Dimensional Graphics and Realism Animation; J5 [Art and Humanities]: Per-forming ArtsMusic Keywords character animation,ac-celerometer control,interactive system,motion capture,music analysis},
   title = {Human-Computer Dance Interaction with Realtime Accelerometer Control},
   year = {2012},
}
@misc{Evola,
   abstract = {This paper intends to contribute to the multimodal turn-taking literature by presenting data collected in an improvisation session in the context of the performing arts and its quali-quantitative analysis, where the focus is on how gaze and the full body participate in the interaction. Five expert performers joined Portuguese contemporary choreographer, João Fi-adeiro, in practicing his Real Time Composition Method during an improvisation session, which was recorded and annotated for this study. A micro-analysis of portions of the session was conducted using ELAN. We found that intersubjectiv-ity was avoided during this performance, both in the perform-ers' bodily movements and mutual gaze; we extrapolate that peripheral vision was chiefly deployed as a regulating strategy by these experts to coordinate turn-taking. A macro-analysis comparing the data with an analogous one obtained from Non-Performers provides the context for a discussion on multimodal-ity and decision-making.},
   author = {Vito Evola and Joanna Skubisz and Carla Fernandes},
   keywords = {Index Terms: gaze,decision-making,gesture function,inter-rater agreement,non-verbal behavior,performing arts,silent turn-taking},
   title = {The Role of Eye Gaze and Body Movements in Turn-Taking during a Contemporary Dance Improvisation},
}
@article{Thomaz2011,
   abstract = {Articles WINTER 2011 53 T urn taking, or use of reciprocal interactions of engagement , is the foundation of human communication. From the first months of life, infants learn to influence the timing , onset, and duration of turns in face-to-face interactions with caregivers through the use of cues such as eye gaze and vocalizations, and they express significant anxiety when deviations from the expected turn-taking pattern take place (Kaye 1977). Linguistics research with adults suggests turn taking is a dynamic and fluid process, whereby interactive partners alternately engage in the various phases of seizing, holding, and yielding the floor through turns and backchannels (Duncan 1974, Orestrom 1983). In order for socially situated, embodied machines to interact with humans properly, it seems logical that they should follow the same deeply rooted turn-taking principles that govern human social behavior. Current interactions with robots are often rigid, ambiguous, and confusing. Humans issuing commands to robots often need to repeat themselves or wait for extended periods of time. Typically robots are unresponsive, opaque machines, and thus frustrating to deal with. The research goal in human-robot interaction (HRI) of unstructured dialogic interaction would allow communication with robots that is as natural as communication with other humans. While the field of HRI tends to focus on speech, gesture, and the content of interaction, our work additionally aims to understand how robots can get the underlying timing of social interaction right. Our overall research agenda is to devise a turn-taking framework for HRI that, like the human skill, represents something fundamental about interaction, generic to context or domain. In this paper we first present our model of floor passing for a human-robot dyad and an experiment with human subjects to derive some of the parameters of this model. A primary conclusion of our data analysis is that human turn-taking behavior is dictated by information flow, and much of the robot's awkward Turn taking is a fundamental part of human communication. Our goal is to devise a turn-taking framework for human-robot interaction that, like the human skill, represents something fundamental about interaction, generic to context or domain. We propose a model of turn taking , and conduct an experiment with human subjects to inform this model. Our findings from this study suggest that information flow is an integral part of human floor-passing behavior. Following this, we implement autonomous floor relinquishing on a robot and discuss our insights into the nature of a general turn-taking model for human-robot interaction.},
   author = {Andrea L Thomaz and Crystal Chao},
   issn = {0738-4602},
   note = {Researchers conducted user study to learn about turn-taking in human robot interaction works by having a human and a "robot" play a game of simon says. The researchers used nonnegativity as a indicator for what causes the turn-taking or motion to action flow to occur, since if the reaction time to an indicator is negative, then it would be impossible that it was the causation for the turn-take. This resulted in the concept of Minimum Necessary Information (MNI) as the primary indicator in this setting, as humans can start doing an action after all the necessary words of the sentence are said and the rest is redundant. This however is not in an improvisational setting, as all actions and all cues are pre-set, and it is easy to predict the following words.},
   title = {Turn Taking Based on Information Flow for Fluent Human-Robot Interaction},
   year = {2011},
}
@misc{Skantze2021,
   abstract = {The taking of turns is a fundamental aspect of dialogue. Since it is difficult to speak and listen at the same time, the participants need to coordinate who is currently speaking and when the next person can start to speak. Humans are very good at this coordination, and typically achieve fluent turn-taking with very small gaps and little overlap. Conversational systems (including voice assistants and social robots), on the other hand, typically have problems with frequent interruptions and long response delays, which has called for a substantial body of research on how to improve turn-taking in conversational systems. In this review article, we provide an overview of this research and give directions for future research. First, we provide a theoretical background of the linguistic research tradition on turn-taking and some of the fundamental concepts in theories of turn-taking. We also provide an extensive review of multi-modal cues (including verbal cues, prosody, breathing, gaze and gestures) that have been found to facilitate the coordination of turn-taking in human-human interaction, and which can be utilised for turn-taking in conversational systems. After this, we review work that has been done on modelling turn-taking, including end-of-turn detection, handling of user interruptions, generation of turn-taking cues, and multi-party human-robot interaction. Finally, we identify key areas where more research is needed to achieve fluent turn-taking in spoken interaction between man and machine.},
   author = {Gabriel Skantze},
   doi = {10.1016/j.csl.2020.101178},
   issn = {10958363},
   journal = {Computer Speech and Language},
   keywords = {Dialogue systems,Gaze,Prosody,Social robotics,Turn-taking},
   month = {5},
   publisher = {Academic Press},
   title = {Turn-taking in Conversational Systems and Human-Robot Interaction: A Review},
   volume = {67},
   year = {2021},
}
@article{Zamm2023,
   abstract = {Turn-taking is a feature of many social interactions such as group music-making, where partners must alternate turns with high precision and accuracy. In two studies of musical rhythm coordination, we investigated how joint action partners learn to coordinate the timing of turn-taking. Musically inexperienced individuals learned to tap at the rate of a pacing cue individually or jointly (in turn with a partner), where each tap produced the next tone in a melodic sequence. In Study 1, partners alternated turns every tap, whereas in Study 2 partners alternated turns every two taps. Findings revealed that partners did not achieve the same level of performance accuracy or precision of inter-tap intervals (ITIs) when producing tapping sequences jointly relative to individually, despite showing learning (increased ITI accuracy and precision across the experiment) in both tasks. Strikingly, partners imposed rhythmic patterns onto jointly produced sequences that captured the temporal structure of turns. Together, learning to produce novel temporal sequences in turn with a partner appears to be more challenging than learning to produce the same sequences alone. Critically, partners may impose rhythmic structures onto turn-taking sequences as a strategy for facilitating coordination.},
   author = {Anna Zamm and Stefan Debener and Natalie Sebanz},
   doi = {10.1038/s41598-022-18480-6},
   issn = {20452322},
   issue = {1},
   journal = {Scientific Reports},
   month = {12},
   note = {Turn taking: it is a coordination challenge. Turns must follow a precies temporal or rythmic structure. This is interesting as dance AI models all try to make models perform well in following the music tempo, which aligns with being good at turn taking.<br/><br/>They try to answer the question of, whether turn-taking can be learned quickly from inexperienced people. <br/><br/>Experiment is set out to understand this. Users are tasked to tap to a beat, and through the self-learning process, they wanted to understand if this process will also indirectly improve their ability to coordinate in alternative tapping with a partner.<br/><br/>How can inexperienced individuals learn turn taking fast?<br/>What mode of turn taking is good for this? What concepts are easiest to grasp?},
   pmid = {36828878},
   publisher = {Nature Research},
   title = {The spontaneous emergence of rhythmic coordination in turn taking},
   volume = {13},
   year = {2023},
}
@misc{Winston2017,
   abstract = {Turn-taking is the ability for agents to lead or follow in social interactions. Turn-taking between humans and intelligent agents has been studied in human-robot interaction but has not been applied to improvisational, dance-based interactions. User understanding and experience of turn-taking in an improvisational, dance-based system known as LuminAI was investigated in a preliminary study of 11 participants. The results showed a trend towards users understanding the difference between turn-taking and non-turn-taking versions of LuminAI but reduced user experience in the turn-taking version.},
   author = {Lauren Winston and Brian Magerko},
   keywords = {Research Papers},
   note = {Winston and Magerko conducted a user study on two versions of the interactive AI agent: "LuminAI". One version, TT-VAI, would have a perception of leadership state, as in who is leading and who is following. While the base version simply mimics or dances similarly to the user when active, and randomly when the user stops being "interesting". An important point raised was how a leadership state is conducted, perceived, and transfered during the course of the dance. Leaders of a dance would likely establish dance moves and a follower would mimic, dance similarly to, or dance differently from the leader. The perception of leadership state is also mentioned, as different dancers could have different perception of who is leader and who is follower, resulting in multiple leaders or multiple followers. This however, is not a concern for LuminAI as the agent only cares about their own perception of leadership state. The transfer of leadership state usually uses cues, and dancing has specific motion cues that dancers traditionally use, like heptic, gaze, and energy (enthusiasm). However, heptic and gaze are challenging to infer hence ignored by LuminAI when making decision about the current leadership state. Enthusiasm is infered from energy, temp and size which can direct the AI agent in decision making. LuminAI also attempts to rengage the user when the user seems disengaged as a way for a leader to mantain their leadership state, and LuminAI can also decide to give up the leadership randomly or based on user's energy. Out of the two versions of the model, the turn taking model was disliked by a few users, who prefered less "back-and-forth" and prefered more natural interactions and felt more inspired. 2 users prefered the turn-taking one as it was providing more than just copying. Mimicery by the agent, however, showed positive feedback from several participants of the user study, since the agent is deemed "more responsibe to my movement." The research highlighted interesting ideas in how a sense of leadership state effect's the user's perception of the model, and how mimicery or being a follower by the model can provide benefits for improved user perception.},
   title = {Turn-Taking with Improvisational Co-Creative Agents},
   url = {www.aaai.org},
   year = {2017},
}
@book{,
   abstract = {"This year the name of the conference changed since the highly successful 12-year IEEE Symposium on 3D User Interfaces is brought back into the main VR conference"--General Chair message, page xviii "IEEE Part Number: CFP18VIR-ART"--PDF copyright page},
   author = {Kiyoshi Kiyokawa and IEEE Computer Society. Technical Committee on Visualization and Graphics and Institute of Electrical and Electronics Engineers},
   isbn = {9781538633656},
   title = {25th IEEE Conference on Virtual Reality and 3D User Interfaces : proceedings : Reutlingen, Germany, 18-22 March 2018},
}
@article{Men2022,
   abstract = {Creating realistic characters that can react to the users' or another character's movement can benefit computer graphics, games and virtual reality hugely. However, synthesizing such reactive motions in human-human interactions is a challenging task due to the many different ways two humans can interact. While there are a number of successful researches in adapting the generative adversarial network (GAN) in synthesizing single human actions, there are very few on modeling human-human interactions. In this paper, we propose a semi-supervised GAN system that synthesizes the reactive motion of a character given the active motion from another character. Our key insights are twofold. First, to effectively encode the complicated spatial-temporal information of a human motion, we empower the generator with a part-based long short-term memory (LSTM) module, such that the temporal movement of different limbs can be effectively modeled. We further include an attention module such that the temporal significance of the interaction can be learned, which enhances the temporal alignment of the active-reactive motion pair. Second, as the reactive motion of different types of interactions can be significantly different, we introduce a discriminator that not only tells if the generated movement is realistic or not, but also tells the class label of the interaction. This allows the use of such labels in supervising the training of the generator. We experiment with the SBU, the HHOI and the 2C datasets. The high quality of the synthetic motion demonstrates the effective design of our generator, and the discriminability of the synthesis also demonstrates the strength of our discriminator.},
   author = {Qianhui Men and Hubert P H Shum and Edmond S L Ho and Howard Leung},
   doi = {10.1016/j.cag.2021.09.014},
   journal = {Computers & Graphics},
   keywords = {Attention,Generative adversarial network,Reactive motion synthesis},
   pages = {634-645},
   title = {GAN-based reactive motion synthesis with class-aware discriminators for human-human interaction},
   volume = {102},
   url = {https://doi.org/10.1016/j.cag.2021.09.014},
   year = {2022},
}
@inproceedings{Liu2019,
   abstract = {Computers that are able to collaboratively improvise movement with humans could have an impact on a variety of application domains, ranging from improving procedural animation in game environments to fostering human-computer co-creativity. Enabling real-time movement improvisation requires equipping computers with strategies for learning and understanding movement. Most existing research focuses on gesture classification, which does not facilitate the learning of new gestures, thereby limiting the creative capacity of computers. In this paper, we explore how to develop a gesture clustering pipeline that facilitates reasoning about arbitrary novel movements in real-time. We describe the implementation of this pipeline within the context of LuminAI, a system in which humans can collaboratively improvise movements together with an AI agent. A preliminary evaluation indicates that our pipeline is capable of efficiently clustering similar gestures together, but further work is necessary to fully assess the pipeline's ability to meaningfully cluster complex movements.},
   author = {Lucas Liu and Duri Long and Swar Gujrania and Brian Magerko},
   doi = {10.1145/3347122.3347127},
   isbn = {9781450376549},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Clustering,Co-creative,Dance,Dimensionality reduction,Dynamic programming,Kinect,Lifelong machine learning,Machine learning,Motion capture,Movement,Pre-processing},
   month = {10},
   publisher = {Association for Computing Machinery},
   title = {Learning movement through human-computer co-creative improvisation},
   year = {2019},
}
@inproceedings{Long2020,
   abstract = {LuminAI is an art installation in which participants can improvise movements with an AI dance partner. In this practice work, we will present the LuminAI installation as well as two visualization tools that interactively demonstrate how the LuminAI agent reasons about movement using both bottom-up learned knowledge and top-down domain knowledge. Participants will first be invited to interact with the LuminAI installation, where they can improvise movement with an AI agent projected onto a screen. They can then see how LuminAI learns relationships between gestures by interacting with MoViz, a visualization in which participants can explore the agent's gesture memory and qualitatively compare the efficacy of unsupervised learning algorithms at clustering gestures. Finally, participants will be invited to interact with a third tool, where they can explore how LuminAI applies top-down domain knowledge to gesture reasoning. Participants will be able to interactively explore how LuminAI uses Laban Movement Analysis's conception of Space to analyze learned movements in terms of the geometric properties of Laban's icosahedron and manipulate these properties to transform and generate new movements. The two visualization tools both represent novel approaches to understanding and analyzing improvisational movement in creative domains.},
   author = {Duri Long and Lucas Liu and Swar Gujrania and Cassandra Naomi and Brian Magerko},
   doi = {10.1145/3401956.3404258},
   isbn = {9781450375054},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Laban movement analysis,artificial intelligence,computational creativity,explainable AI,gesture,gesture clustering,motion analysis,movement improvisation,unsupervised learning,visualization},
   month = {7},
   publisher = {Association for Computing Machinery},
   title = {Visualizing Improvisation in LuminAI, an AI Partner for Co-Creative Dance},
   year = {2020},
}
@article{Tseng2022,
   abstract = {Dance is an important human art form, but creating new dances can be difficult and time-consuming. In this work, we introduce Editable Dance GEneration (EDGE), a state-of-the-art method for editable dance generation that is capable of creating realistic, physically-plausible dances while remaining faithful to the input music. EDGE uses a transformer-based diffusion model paired with Jukebox, a strong music feature extractor, and confers powerful editing capabilities well-suited to dance, including joint-wise conditioning, and in-betweening. We introduce a new metric for physical plausibility, and evaluate dance quality generated by our method extensively through (1) multiple quantitative metrics on physical plausibility, beat alignment, and diversity benchmarks, and more importantly, (2) a large-scale user study, demonstrating a significant improvement over previous state-of-the-art methods. Qualitative samples from our model can be found at our website.},
   author = {Jonathan Tseng and Rodrigo Castellon and C. Karen Liu},
   month = {11},
   title = {EDGE: Editable Dance Generation From Music},
   url = {http://arxiv.org/abs/2211.10658},
   year = {2022},
}
@article{Li2021,
   abstract = {We present AIST++, a new multi-modal dataset of 3D dance motion and music, along with FACT, a Full-Attention Cross-modal Transformer network for generating 3D dance motion conditioned on music. The proposed AIST++ dataset contains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance genres with multi-view videos with known camera poses -- the largest dataset of this kind to our knowledge. We show that naively applying sequence models such as transformers to this dataset for the task of music conditioned 3D motion generation does not produce satisfactory 3D motion that is well correlated with the input music. We overcome these shortcomings by introducing key changes in its architecture design and supervision: FACT model involves a deep cross-modal transformer block with full-attention that is trained to predict $N$ future motions. We empirically show that these changes are key factors in generating long sequences of realistic dance motion that are well-attuned to the input music. We conduct extensive experiments on AIST++ with user studies, where our method outperforms recent state-of-the-art methods both qualitatively and quantitatively.},
   author = {Ruilong Li and Shan Yang and David A. Ross and Angjoo Kanazawa},
   month = {1},
   title = {AI Choreographer: Music Conditioned 3D Dance Generation with AIST++},
   url = {http://arxiv.org/abs/2101.08779},
   year = {2021},
}
@misc{,
   abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension , and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset-matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
   author = {Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
   title = {Language Models are Unsupervised Multitask Learners},
   url = {https://github.com/codelucas/newspaper},
}
@inproceedings{Yao2023,
   abstract = {Recently, digital humans for interpersonal interaction in virtual environments have gained significant attention. In this paper, we introduce a novel multi-dancer synthesis task called partner dancer generation, which involves synthesizing virtual human dancers capable of performing dance with users. The task aims to control the pose diversity between the lead dancer and the partner dancer. The core of this task is to ensure the controllable diversity of the generated partner dancer while maintaining temporal coordination with the lead dancer. This scenario varies from earlier research in generating dance motions driven by music, as our emphasis is on automatically designing partner dancer postures according to pre-defined diversity, the pose of lead dancer, as well as the accompanying tunes. To achieve this objective, we propose a three-stage framework called Dance-with-You (DanY). Initially, we employ a 3D Pose Collection stage to collect a wide range of basic dance poses as references for motion generation. Then, we introduce a hyper-parameter that coordinates the similarity between dancers by masking poses to prevent the generation of sequences that are over-diverse or consistent. To avoid the rigidity of movements, we design a Dance Pre-generated stage to pre-generate these masked poses instead of filling them with zeros. After that, a Dance Motion Transfer stage is adopted with leader sequences and music, in which a multi-conditional sampling formula is rewritten to transfer the pre-generated poses into a sequence with a partner style. In practice, to address the lack of multi-person datasets, we introduce AIST-M, a new dataset for partner dancer generation, which is publicly availiable at https://github.com/JJessicaYao/AIST-M-Dataset. Comprehensive evaluations on our AIST-M dataset demonstrate that the proposed DanY can synthesize satisfactory partner dancer results with controllable diversity.},
   author = {Siyue Yao and Mingjie Sun and Bingliang Li and Fengyu Yang and Junle Wang and Ruimao Zhang},
   doi = {10.1145/3581783.3612046},
   isbn = {9798400701085},
   journal = {MM 2023 - Proceedings of the 31st ACM International Conference on Multimedia},
   keywords = {diffusion model,diversity controllability,partner dancer synthesis},
   month = {10},
   note = {Implementation: <br/>- 3 stages:<br/><br/>1. VQVAE (same as the bailando model I believe)<br/>   Take dance sequence: S, where S -> R^(T, J, 3), T being time series dimension, and J is the joint information dimension. Turn S into Q, where Q -> R6(T, F), where T is time series dim, and F is features dim. TLDR; turn complicated 3D dance joint position data into pose codes that can be learned easier by the following blocks.<br/><br/>2. Dance Pregen<br/>   Take the lead dancer dance sequence, randomly mask out X number of frames using a preset value. Fill the frames with noise and denoise with a [Transformer?] U-Net. Then output to next stage. U-Net is made up of 2 Transformer and 5 resnets as encoder and decoder.<br/><br/>3. Motion Transfer<br/>   Use the same feature selection (mask) module to select the lead features, and masked music features, using the reverse mask. Then, take both feature sets, and use as one condition for the denoise U-Net and the previous layer's output as another condition. Loss for the U-Net: pg 5.<br/>},
   pages = {8504-8514},
   publisher = {Association for Computing Machinery, Inc},
   title = {Dance with You: The Diversity Controllable Dancer Generation via Diffusion Models},
   year = {2023},
}
@inproceedings{Alemi2017,
   author = {Omid Alemi and Jules Françoise and Philippe Pasquier},
   month = {4},
   title = {GrooveNet: Real-Time Music-Driven Dance Movement Generation using Artificial Neural Networks},
   year = {2017},
}
@inproceedings{Wang2022,
   abstract = {Different people dance in different styles. So when multiple people dance together, the phenomenon of style collaboration occurs: people need to seek common points while reserving differences in various dancing periods. Thus, we introduce a novel Music-driven Group Dance Synthesis task. Compared with single-people dance synthesis explored by most previous works, modeling the style collaboration phenomenon and choreographing for multiple people are more complicated and challenging. Moreover, the lack of sufficient records for conducting multi-people choreography in prior datasets further aggravates this problem. To address these issues, we construct a rich-annotated 3D Multi-Dancer Choreography dataset (MDC) and newly devise a metric SCEU for style collaboration evaluation. To our best knowledge, MDC is the first 3D dance dataset that collects both individual and collaborated music-dance pairs. Based on MDC, we present a novel framework, GroupDancer, consisting of three stages: Dancer Collaboration, Motion Choreography and Motion Transition. The Dancer Collaboration stage determines when and which dancers should collaborate their dancing styles from music. Afterward, the Motion Choreography stage produces a motion sequence for each dancer. Finally, the Motion Transition stage fills the gaps between the motions to achieve fluent and natural group dance. To make GroupDancer trainable from end to end and able to synthesize group dance with style collaboration, we propose mixed training and selective updating strategies. Comprehensive evaluations on the MDC dataset demonstrate that the proposed GroupDancer model can synthesize quite satisfactory group dance synthesis results with style collaboration.},
   author = {Zixuan Wang and Jia Jia and Haozhe Wu and Junliang Xing and Jinghe Cai and Fanbo Meng and Guowen Chen and Yanfeng Wang},
   city = {New York, NY, USA},
   doi = {10.1145/3503161.3548090},
   isbn = {9781450392037},
   journal = {Proceedings of the 30th ACM International Conference on Multimedia},
   keywords = {choreography,group dance synthesis,style collaboration},
   pages = {1138-1146},
   publisher = {Association for Computing Machinery},
   title = {GroupDancer: Music to Multi-People Dance Synthesis with Style Collaboration},
   url = {https://doi.org/10.1145/3503161.3548090},
   year = {2022},
}
@article{Siyao2023,
   author = {Li Siyao and Weijiang Yu and Tianpei Gu and Chunze Lin and Quan Wang and Chen Qian and Chen Change Loy and Ziwei Liu},
   doi = {10.1109/TPAMI.2023.3319435},
   issue = {12},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   keywords = {Humanities;Three-dimensional displays;Codes;Transformers;Avatars;Rhythm;Encoding;3D human motion;dance generation;GPT;multi-modal;VQ-VAE},
   note = {model: VQVAE into a Transformer<br/><br/>VQVAE is a solution for the problem of low spacial quality, performing physically infeasible 3D motion.<br/><br/>Training the VQVAE decoder cannot be trained to generate rot mat by itself.<br/><br/>Bailando++ does not learn the mapping between music and dance directly},
   pages = {14192-14207},
   title = {Bailando++: 3D Dance GPT With Choreographic Memory},
   volume = {45},
   year = {2023},
}
@inproceedings{Kim2022,
   author = {Jinwoo Kim and Heeseok Oh and Seongjean Kim and Hoseok Tong and Sanghoon Lee},
   doi = {10.1109/CVPR52688.2022.00348},
   journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
   keywords = {Training;Measurement;Humanities;Computer vision;Computational modeling;Computer architecture;Transformers;Image and video synthesis and generation; Action and event recognition; Deep learning architectures and techniques; Vision + X},
   pages = {3480-3490},
   title = {A Brand New Dance Partner: Music-Conditioned Pluralistic Dancing Controlled by Multiple Dance Genres},
   year = {2022},
}
@inproceedings{Huang2022,
   author = {Yuhang Huang and Junjie Zhang and Shuyan Liu and Qian Bao and Dan Zeng and Zhineng Chen and Wu Liu},
   doi = {10.1109/ICASSP43922.2022.9747838},
   journal = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
   keywords = {Solid modeling;Three-dimensional displays;Conferences;Music;Signal processing;Transformers;Decoding;3D dance generation;genre-conditioned;modality fusion;music-driven},
   pages = {4858-4862},
   title = {Genre-Conditioned Long-Term 3D Dance Generation Driven by Music},
   year = {2022},
}
@inproceedings{Trajkova2023,
   abstract = {In this demonstration, we present a holographic projected version of LuminAI, which is an interactive art installation that allows participants to collaborate with an AI dance partner by improvising movements together. By utilizing a mix of a top-down and bottom-up approach, we seek to understand embodied co-creativity in an improvisational dance setting to better develop the design of the modular AI agent to creatively collaborate with a dancer. The purpose of this demonstration is to describe the five-module agent design and investigate how we can design an immersive experience that is design-efficient, portable, light, and duo-user participation. Through this installation in an imitated black box space, audience members and dancers engage in an immersive co-creative dance experience, inspiring discussion on the limitless applications of dance and technology in the realms of learning, training, and creativity.},
   author = {Milka Trajkova and Manoj Deshpande and Andrea Knowlton and Cassandra Monden and Duri Long and Brian Magerko},
   city = {New York, NY, USA},
   doi = {10.1145/3563703.3596658},
   isbn = {9781450398985},
   journal = {Companion Publication of the 2023 ACM Designing Interactive Systems Conference},
   keywords = {AI agents,co-creative AI,co-creative agents,co-creativity,dance improvisation},
   pages = {274-278},
   publisher = {Association for Computing Machinery},
   title = {AI Meets Holographic Pepper’s Ghost: A Co-Creative Public Dance Experience},
   url = {https://doi.org/10.1145/3563703.3596658},
   year = {2023},
}
@article{Pettee2019,
   author = {Mariel Pettee and Chase Shimmin and Douglas Duhaime and Ilya Vidrin},
   title = {Beyond Imitation: Generative and Variational Choreography via Machine Learning},
   year = {2019},
}
@article{,
   abstract = {Technologies for sensing movement are expanding toward everyday use in virtual reality, gaming, and artistic practices. In this context, there is a need for methodologies to help designers and users create meaningful movement experiences. This article discusses a user-centered approach for the design of interactive auditory feedback using interactive machine learning. We discuss Mapping through Interaction, a method for crafting sonic interactions from corporeal demonstrations of embodied associations between motion and sound. It uses an interactive machine learning approach to build the mapping from user demonstrations, emphasizing an iterative design process that integrates acted and interactive experiences of the relationships between movement and sound. We examine Gaussian Mixture Regression and Hidden Markov Regression for continuous movement recognition and real-time sound parameter generation. We illustrate and evaluate this approach through an application in which novice users can create interactive sound feedback based on coproduced gestures and vocalizations. Results indicate that Gaussian Mixture Regression and Hidden Markov Regression can efficiently learn complex motion-sound mappings from few examples.},
   author = {Jules Françoise and Frédéric Bevilacqua},
   city = {New York, NY, USA},
   doi = {10.1145/3211826},
   issn = {2160-6455},
   issue = {2},
   journal = {ACM Trans. Interact. Intell. Syst.},
   keywords = {Interactive machine learning,movement,programming-by-demonstration,sonification,sound and music computing,user-centered design},
   month = {6},
   publisher = {Association for Computing Machinery},
   title = {Motion-Sound Mapping through Interaction: An Approach to User-Centered Design of Auditory Feedback Using Machine Learning},
   volume = {8},
   url = {https://doi.org/10.1145/3211826},
   year = {2018},
}
@inproceedings{Wallace2021,
   abstract = {Through dance, a wide range of emotions can be expressed. As virtual agents and robots continue to become part of our daily lives, the need for them to efficiently convey emotion and intent increases. When trained to dance, to what extent can AI learn to model the tacit mappings between sound and motion? Here, we explore the creative capacity of a generative model trained on 3D motion capture recordings of improvised dance. We perform a perceptual judgment experiment wherein respondents rate movement generated by our model as well as human performances. While the sound-motion mappings remain somewhat elusive, particularly when compared to examples of human dance, our study shows that in certain aspects related to perceived dance-likeness and expressivity, the model successfully mimics human dance movement. By employing a perceptual study to evaluate our generative model, we aim to further our ability to understand the affordances and limitations of creative AI.},
   author = {Benedikte Wallace and Charles P Martin and Jim T\o\{\}rresen and Kristian Nymoen},
   city = {New York, NY, USA},
   doi = {10.1145/3450741.3465245},
   isbn = {9781450383769},
   journal = {Proceedings of the 13th Conference on Creativity and Cognition},
   keywords = {Dance,Embodied Music Cognition,Generative AI,Mixture Density Networks,Perceptual judgement experiment},
   publisher = {Association for Computing Machinery},
   title = {Learning Embodied Sound-Motion Mappings: Evaluating AI-Generated Dance Improvisation},
   url = {https://doi.org/10.1145/3450741.3465245},
   year = {2021},
}
@inproceedings{Wallace2023,
   abstract = {What expectations exist in the minds of dancers when interacting with a generative machine learning model? During two workshop events, experienced dancers explore these expectations through improvisation and role-play, embodying an imagined AI-dancer. The dancers explored how intuited flow, shared images, and the concept of a human replica might work in their imagined AI-human interaction. Our findings challenge existing assumptions about what is desired from generative models of dance, such as expectations of realism, and how such systems should be evaluated. We further advocate that such models should celebrate non-human artefacts, focus on the potential for serendipitous moments of discovery, and that dance practitioners should be included in their development. Our concrete suggestions show how our findings can be adapted into the development of improved generative and interactive machine learning models for dancers’ creative practice.},
   author = {Benedikte Wallace and Clarice Hilton and Kristian Nymoen and Jim Torresen and Charles Patrick Martin and Rebecca Fiebrink},
   city = {New York, NY, USA},
   doi = {10.1145/3591196.3593336},
   isbn = {9798400701801},
   journal = {Proceedings of the 15th Conference on Creativity and Cognition},
   keywords = {dance,embodiment,generative AI,reflexive thematic analysis},
   note = {The dancers would move in similar ways, but would rarely mimic their partner’s movements directly. Instead, their movements were often inspired by their partner, taking on certain characteristics such as the trajectories of the arms or mirroring their use of space,<br/><br/>Shared images, like shared references are a part of co-creating that some dancers don't expect AI to capture well.<br/><br/>Idocyncrasies are challenging for AIs to capture. This is true for image generation AI models.<br/><br/>When running out of inspiration, they expect their AI partners to bring something new to the table, "opposite of what they are doing"<br/><br/>shared images, or a rappor helps dancers predict what their partners will do next and improvise based on intuition. <br/><br/>Improvisational dancers want their partner to shift the direction of their improvisation},
   pages = {454-464},
   publisher = {Association for Computing Machinery},
   title = {Embodying an Interactive AI for Dance Through Movement Ideation},
   url = {https://doi.org/10.1145/3591196.3593336},
   year = {2023},
}
@inproceedings{Long2019,
   abstract = {Artificial intelligence (AI) is becoming increasingly pervasive in our everyday lives. There are consequently many common misconceptions about what AI is, what it is capable of, and how it works. Compounding the issue, opportunities to learn about AI are often limited to audiences who already have access to and knowledge about technology. Increasing access to AI in public spaces has the potential to broaden public AI literacy, and experiences involving co-creative (i.e. collaboratively creative) AI are particularly well-suited for engaging a broad range of participants. This paper explores how to design co-creative AI for public interaction spaces, drawing both on existing literature and our own experiences designing co-creative AI for public venues. It presents a set of design principles that can aid others in the development of co-creative AI for public spaces as well as guide future research agendas.},
   author = {Duri Long and Mikhail Jacob and Brian Magerko},
   city = {New York, NY, USA},
   doi = {10.1145/3325480.3325504},
   isbn = {9781450359177},
   journal = {Proceedings of the 2019 Conference on Creativity and Cognition},
   keywords = {co-creative ai,collaboration,human-centered ai,public displays,reflection on design processes},
   pages = {271-284},
   publisher = {Association for Computing Machinery},
   title = {Designing Co-Creative AI for Public Spaces},
   url = {https://doi.org/10.1145/3325480.3325504},
   year = {2019},
}
@article{Sun2021,
   author = {Guofei Sun and Yongkang Wong and Zhiyong Cheng and Mohan S Kankanhalli and Weidong Geng and Xiangdong Li},
   doi = {10.1109/TMM.2020.2981989},
   journal = {IEEE Transactions on Multimedia},
   keywords = {Generators;Feature extraction;Task analysis;Correlation;Three-dimensional displays;Music;Deep learning;Music-driven dance choreography;adversarial learning;cross-modal association},
   pages = {497-509},
   title = {DeepDance: Music-to-Dance Motion Choreography With Adversarial Learning},
   volume = {23},
   year = {2021},
}
@article{Holden2016,
   abstract = {We present a framework to synthesize character movements based on high level parameters, such that the produced movements respect the manifold of human motion, trained on a large motion capture dataset. The learned motion manifold, which is represented by the hidden units of a convolutional autoencoder, represents motion data in sparse components which can be combined to produce a wide range of complex movements. To map from high level parameters to the motion manifold, we stack a deep feedforward neural network on top of the trained autoencoder. This network is trained to produce realistic motion sequences from parameters such as a curve over the terrain that the character should follow, or a target location for punching and kicking. The feedforward control network and the motion manifold are trained independently, allowing the user to easily switch between feedforward networks according to the desired interface, without re-training the motion manifold. Once motion is generated it can be edited by performing optimization in the space of the motion manifold. This allows for imposing kinematic constraints, or transforming the style of the motion, while ensuring the edited motion remains natural. As a result, the system can produce smooth, high quality motion sequences without any manual pre-processing of the training data.},
   author = {Daniel Holden and Jun Saito and Taku Komura},
   city = {New York, NY, USA},
   doi = {10.1145/2897824.2925975},
   issn = {0730-0301},
   issue = {4},
   journal = {ACM Trans. Graph.},
   keywords = {autoencoder,character animation,convolutional neural networks,deep learning,human motion,manifold learning},
   month = {7},
   publisher = {Association for Computing Machinery},
   title = {A deep learning framework for character motion synthesis and editing},
   volume = {35},
   url = {https://doi.org/10.1145/2897824.2925975},
   year = {2016},
}
@inproceedings{Tang2018,
   abstract = {Dance is greatly influenced by music. Studies on how to synthesize music-oriented dance choreography can promote research in many fields, such as dance teaching and human behavior research. Although considerable effort has been directed toward investigating the relationship between music and dance, the synthesis of appropriate dance choreography based on music remains an open problem. There are two main challenges: 1) how to choose appropriate dance figures, i.e., groups of steps that are named and specified in technical dance manuals, in accordance with music and 2) how to artistically enhance choreography in accordance with music. To solve these problems, in this paper, we propose a music-oriented dance choreography synthesis method using a long short-term memory (LSTM)-autoencoder model to extract a mapping between acoustic and motion features. Moreover, we improve our model with temporal indexes and a masking method to achieve better performance. Because of the lack of data available for model training, we constructed a music-dance dataset containing choreographies for four types of dance, totaling 907,200 frames of 3D dance motions and accompanying music, and extracted multidimensional features for model training. We employed this dataset to train and optimize the proposed models and conducted several qualitative and quantitative experiments to select the best-fitted model. Finally, our model proved to be effective and efficient in synthesizing valid choreographies that are also capable of musical expression.},
   author = {Taoran Tang and Jia Jia and Hanyang Mao},
   city = {New York, NY, USA},
   doi = {10.1145/3240508.3240526},
   isbn = {9781450356657},
   journal = {Proceedings of the 26th ACM International Conference on Multimedia},
   keywords = {3d motion capture,autoencoder,lstm,motion synthesis,music-dance dataset},
   pages = {1598-1606},
   publisher = {Association for Computing Machinery},
   title = {Dance with Melody: An LSTM-autoencoder Approach to Music-oriented Dance Synthesis},
   url = {https://doi.org/10.1145/3240508.3240526},
   year = {2018},
}
@misc{Ahn2019,
   author = {Hyemin Ahn and Jaehun Kim and Kihyun Kim and Songhwai Oh},
   title = {Generative Autoregressive Networks for 3D Dancing Move Synthesis from Music},
   year = {2019},
}
@misc{Li2023,
   author = {Buyu Li and Yongchi Zhao and Zhelun Shi and Lu Sheng},
   title = {DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer},
   year = {2023},
}
@article{Li2020,
   author = {Jiaman Li and Yihang Yin and Hang Chu and Yi Zhou and Tingwu Wang and Sanja Fidler and Hao Li},
   title = {Learning to Generate Diverse Dance Motions with Transformer},
   year = {2020},
}
@article{Lee2019,
   author = {Hsin-Ying Lee and Xiaodong Yang and Ming-Yu Liu and Ting-Chun Wang and Yu-Ding Lu and Ming-Hsuan Yang and Jan Kautz},
   title = {Dancing to Music},
   year = {2019},
}
@inproceedings{Bisig2022,
   author = {Daniel Bisig and Ephraim Wegner},
   month = {4},
   title = {Puppeteering AI -Interactive Control of an Artificial Dancer},
   year = {2022},
}
@inproceedings{Qi2023,
   abstract = {When hearing music, it is natural for people to dance to its rhythm. Automatic dance generation, however, is a challenging task due to the physical constraints of human motion and rhythmic alignment with target music. Conventional autoregressive methods introduce compounding errors during sampling and struggle to capture the long-term structure of dance sequences. To address these limitations, we present a novel cascaded motion diffusion model, DiffDance, designed for high-resolution, long-form dance generation. This model comprises a music-to-dance diffusion model and a sequence super-resolution diffusion model. To bridge the gap between music and motion for conditional generation, DiffDance employs a pretrained audio representation learning model to extract music embeddings and further align its embedding space to motion via contrastive loss. During training our cascaded diffusion model, we also incorporate multiple geometric losses to constrain the model outputs to be physically plausible and add a dynamic loss weight that adaptively changes over diffusion timesteps to facilitate sample diversity. Through comprehensive experiments performed on the benchmark dataset AIST++, we demonstrate that DiffDance is capable of generating realistic dance sequences that align effectively with the input music. These results are comparable to those achieved by state-of-the-art autoregressive methods.},
   author = {Qiaosong Qi and Le Zhuo and Aixi Zhang and Yue Liao and Fei Fang and Si Liu and Shuicheng Yan},
   city = {New York, NY, USA},
   doi = {10.1145/3581783.3612307},
   isbn = {9798400701085},
   journal = {Proceedings of the 31st ACM International Conference on Multimedia},
   keywords = {conditional generation,diffusion model,multimodal learning,music-to-dance},
   note = {Model: cascaded (likely unet upsampling) motion duffusion model<br/><br/>Dataset: AIST++<br/><br/>- autoregressive methods introduce compounding errors during sampling and struggle to capture long-term structure.<br/>- Uses wave2CLIP instead of Librosa for music feature extraction},
   pages = {1374-1382},
   publisher = {Association for Computing Machinery},
   title = {DiffDance: Cascaded Human Motion Diffusion Model for Dance Generation},
   url = {https://doi.org/10.1145/3581783.3612307},
   year = {2023},
}
@article{Le2023,
   abstract = {Music-driven group choreography poses a considerable challenge but holds significant potential for a wide range of industrial applications. The ability to generate synchronized and visually appealing group dance motions that are aligned with music opens up opportunities in many fields such as entertainment, advertising, and virtual performances. However, most of the recent works are not able to generate high-fidelity long-term motions, or fail to enable controllable experience. In this work, we aim to address the demand for high-quality and customizable group dance generation by effectively governing the consistency and diversity of group choreographies. In particular, we utilize a diffusion-based generative approach to enable the synthesis of flexible number of dancers and long-term group dances, while ensuring coherence to the input music. Ultimately, we introduce a Group Contrastive Diffusion (GCD) strategy to enhance the connection between dancers and their group, presenting the ability to control the consistency or diversity level of the synthesized group animation via the classifier-guidance sampling technique. Through intensive experiments and evaluation, we demonstrate the effectiveness of our approach in producing visually captivating and consistent group dance motions. The experimental results show the capability of our method to achieve the desired levels of consistency and diversity, while maintaining the overall quality of the generated group choreography.},
   author = {Nhat Le and Tuong Do and Khoa Do and Hien Nguyen and Erman Tjiputra and Quang D Tran and Anh Nguyen},
   city = {New York, NY, USA},
   doi = {10.1145/3618356},
   issn = {0730-0301},
   issue = {6},
   journal = {ACM Trans. Graph.},
   keywords = {diffusion models,group choreography animation,group motion synthesis,machine learning},
   month = {12},
   publisher = {Association for Computing Machinery},
   title = {Controllable Group Choreography Using Contrastive Diffusion},
   volume = {42},
   url = {https://doi.org/10.1145/3618356},
   year = {2023},
}
@article{Pettee2019,
   author = {Mariel Pettee and Chase Shimmin and Douglas Duhaime and Ilya Vidrin},
   title = {Beyond Imitation: Generative and Variational Choreography via Machine Learning},
   year = {2019},
}
