\documentclass[final,5p,times,authoryear]{article}
\usepackage{cite}
\begin{document}

\title{Living Document}
\author{Jiazhi Zhou}
\maketitle

\begin{abstract}

This document is used to track progress of the experiments, and literatures read.

\end{abstract}

\section{Introduction}
% a few motivating statements

TODO: write.

\section{Related Work}
% a list of all papers you’ve read, in clusters of similarity, with a few comments on what it was, what was relevant for your project
% some papers will be less relevant, used for context, other papers that are more relevant you can spend more space describing

TODO: write.

\section{Project Context}
% briefly describe goal to support embodied improvisation in VR, for performance, public installation, home use, equipment, etc.
% requirements for ai agent

Goal:

The goal of this project is to explore the capabilities of generative AI models
in supporting various embodied creativity exercises. These include embodied
dance improvisations, performances, public installations and so on.

LumnAI (add ref) is used for public installation. However it is a algorithm and
not a generative deep learning model.

requirements:

For the generative AI to add to the embodied creativity, there are some
requirements that the AI model has to fulfill.

1. Run fast
The AI model has to be able to run quickly. As most machine learning clusters
are not connected to the internet, and would be useless for actively using for a
project like this.

The model has to be able to run on a standalone device, such as a PC or a
laptop. This requires the model to be small enough to load, and run fast enough
so the user feels minimal lag in their input and the models' outuput

2. Small Size
If the model is too large, it would not load on a easily portable device, like a
laptop or a PC, making it impossible to be used.

3. Take in user input
This requirement takes affect in two senses. One is that, the model has to have
the user input as a condition that it generates on. The other is that the model
has to be able to process any user input.

Machine learning model tends to focus on the data that they are trained on, and
if a model only generates well on its training data, it would not be good for a
public installation, as we can not predict user movements.

\section{Procedures}
% what AI agents you’ve explored so far, main training set and characteristics of each
% describe the goal for re-seeding of the ai agent, and technical details of doing so, how you’ve extended the ai agent.
% describe the parameter explorations that you’ve done

What AI Agents we have explored so far.

The existing "dance generation" AIs were reviewed, and filtered based on
availability as well as fit for purpose. The models that were publicly
available, and fit our requirement, of generating future dance based on dance
input, were evaluated. The most fit for purpose model was picked to be the
Bailando++ model. The MNET model was also seen as a viable choice but due to its
complications in training, we have deferred the use of this model for the
current research till a later date. The DanY (Dance With You) model is the most
fit for purpose, but the model size and training complexity made it out of scope
for this current project.

\subsection{Bailando++}

Bailando++ is a model that we explored. This model fits our base requirements of
being small, fast to run, and can be configured to have user's dance input as a
generation condition.

\subsubsection{Model Details}
This model uses a VQVAE which turns input dances to discrete pose codes. A
sequence of these codes are put into a transformer block to procude future
dances poses, which is then decoded back into dance.

\subsubsection{In The Wild Generation}
Generating dance with our own input with Bailando is not straight forward, as
Bailando only takes music as a condition when running evaluation. However, there
were steps to be taken to insert dance input to the generator.

The generator at core is a transformer block which we will call the gpt.
The gpt which has a max context length of 28 and starts the generation at 1
which is just a zero value to get started. The gpt then generates each
subsequent pose sequentially until the desired length is reached. 

We converted the code to start the generation at 28, and bring in our own
encoded dance sequence for those 28 pose codes. The gpt then generates the dance
using full context immediately and then keep generating to the desired length.

We also allowed dynamic control of input length, so we can observe how different
input length can affect the output of the model.

\subsubsection{Evaluation}
Initially we captured 7 different pieces of input from the recorded sequence.
This is then generated to full dances using different input length and music. We
used input length of 2, 10, 17, and 28, which roughly equates to 0.3, 1.3, 2.3,
and 3.7 seconds of input.

These results were first intenally evaluated in the research group, and
determined that using the full input length produced the most relevant output.
The group also determined that more music types needed to be attempted. 

From there, we generated dance videos using 12 distinct input clips taken from the kinect
recorded dance seuqences. We also used 6 different pieces of music, including
ambient music with no distinct beat, pop music, classical piano, and a piece
from the AIST dataset. These clips all use a context length of 28, and generates
a full dance of around 10 seconds.

\subsection{MNET}

MNET is a model that uses a GAN architecture to produce future dances. The
pretrained model was not available publicly so we used their public code to
reproduce the training.

\subsubsection{Model Training}

The code had to be modified to fit the training. The hyper-parameters the
researchers used were copied from their paper into the configs for training, and
we used a single A100 GPU to train up to the recommended 500k steps. The output
motion was jittery so further training was ran but the model over-fit the data
heavily after 500k steps.

\subsubsection{In The Wild Generation}
We used MNET to generate videos "in-the-wild" with motion taken from the
recorded sequences from Unity, and music not in the AIST++ dataset. The steps we
took to generate the videos are.

1. using joint2smpl library to convert the 3d joint coordinate data into SMPL
parameters
2. using the SMPL parameters and rendering videos from it to evaluate the motion
quality of the converted motion.
3. after validating the motion resembles the original dance, first 60 seconds of
each clip is used by the model to generate dances with AIST music 
4. the validated motion are then used to generate dances with new music

The resulting dance is of poor quality, especially when using new music. The
first output clip is immediately attempts to regress back to the SMPL mean
position with each clip regressing more and more.

This problem is identified to be over fitting, due to the model working well with
training data but not with unseen data. The new model is trained with reduced
parameter count up to 460k steps with one A100 GPU. The resulting model performs
the same

The problem could be the issue with MNET's ability to recognize malformed
dances, since the input motion, although of good enough quality for human, the
exact data could be lower quality and making the model regress to the mean pose.
However the model will not be fit for use if data needs to be well formed, since
input from the kinect will be low quality sometimes, and the model has to be
able to deal with this quality.

\end{document}