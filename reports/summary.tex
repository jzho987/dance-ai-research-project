\documentclass[final,5p,times,authoryear]{article}
\usepackage{cite}
\begin{document}

\title{Living Document}
\author{Jiazhi Zhou}
\maketitle

\begin{abstract}

This document is used to track progress of the experiments, and literatures read.

\end{abstract}

\section{Introduction}
% a few motivating statements

TODO: write.

\section{Related Work}
% a list of all papers you’ve read, in clusters of similarity, with a few comments on what it was, what was relevant for your project
% some papers will be less relevant, used for context, other papers that are more relevant you can spend more space describing

TODO: write.

\section{Project Context}
% briefly describe goal to support embodied improvisation in VR, for performance, public installation, home use, equipment, etc.
% requirements for ai agent

Goal:

The goal of this project is to explore the capabilities of generative AI models
in supporting various embodied creativity exercises. These include embodied
dance improvisations, performances, public installations and so on.

LumnAI (add ref) is used for public installation. However it is a algorithm and
not a generative deep learning model.

requirements:

For the generative AI to add to the embodied creativity, there are some
requirements that the AI model has to fulfill.

1. Run fast
The AI model has to be able to run quickly. As most machine learning clusters
are not connected to the internet, and would be useless for actively using for a
project like this.

The model has to be able to run on a standalone device, such as a PC or a
laptop. This requires the model to be small enough to load, and run fast enough
so the user feels minimal lag in their input and the models' outuput

2. Small Size
If the model is too large, it would not load on a easily portable device, like a
laptop or a PC, making it impossible to be used. Most machine learning models are
trained in machine learning clusters with A100 GPUs. However, most institutions do
not have access to systems connected to the internet that also has GPUs like the A100.
The easiest and most accessible high power GPUs are the 4000 series GPUs from
Nvidia, and they will have up to 16GB of VRam. Any Machine learning model that do not
fit on 16GB of Vram will not work with a framework like this.

3. Take in user input
This requirement takes affect in two senses. One is that, the model has to have
the user input as a condition that it generates on. The other is that the model
has to be able to process any user input.

Machine learning model tends to focus on the data that they are trained on, and
if a model only generates well on its training data, it would not be good for a
public installation, as we can not predict user movements.

This is also important for the type of input. The AIST++ data set contains both COCO
and SMPL keypoint data. The COCO keypoints are 3D position based, while the SMPL paramter
data are not. To produce SMPL parameter data on the fly is incredibly challenging and takes a long time to fit. Which means it will not work with a real-time interaction framework.

Most dance AI models like to use the SMPL parameters to train. If an AI model wants to fit
the use of a real-time interaction framework, it would work better with COCO format, or SMPL
keypoint position rather than the axis angle rotational format that SMPL paramters take.

\section{Procedures}
% what AI agents you’ve explored so far, main training set and characteristics of each
% describe the goal for re-seeding of the ai agent, and technical details of doing so, how you’ve extended the ai agent.
% describe the parameter explorations that you’ve done

What AI Agents we have explored so far.

The existing "dance generation" AIs were reviewed, and filtered based on
availability as well as fit for purpose. The models that were publicly
available, and fit our requirement, of generating future dance based on dance
input, were evaluated. The most fit for purpose model was picked to be the
Bailando++ model. The MNET model was also seen as a viable choice but due to its
complications in training, we have deferred the use of this model for the
current research till a later date. The DanY (Dance With You) model is the most
fit for purpose, but the model size and training complexity made it out of scope
for this current project.

\subsection{Bailando++}

Bailando++ is a model that we explored. This model fits our base requirements of
being small, fast to run, and can be configured to have user's dance input as a
generation condition.

\subsubsection{Model Details}
This model uses a VQVAE which turns input dances to discrete pose codes. A
sequence of these codes are put into a transformer block to procude future
dances poses, which is then decoded back into dance.

\subsubsection{In The Wild Generation}
Generating dance with our own input with Bailando is not straight forward, as
Bailando only takes music as a condition when running evaluation. However, there
were steps to be taken to insert dance input to the generator.

The generator at core is a transformer block which we will call the gpt.
The gpt which has a max context length of 28 and starts the generation at 1
which is just a zero value to get started. The gpt then generates each
subsequent pose sequentially until the desired length is reached.

We converted the code to start the generation at 28, and bring in our own
encoded dance sequence for those 28 pose codes. The gpt then generates the dance
using full context immediately and then keep generating to the desired length.

We also allowed dynamic control of input length, so we can observe how different
input length can affect the output of the model.

\subsubsection{Evaluation}
Initially we captured 7 different pieces of input from the recorded sequence.
This is then generated to full dances using different input length and music. We
used input length of 2, 10, 17, and 28, which roughly equates to 0.3, 1.3, 2.3,
and 3.7 seconds of input.

These results were first intenally evaluated in the research group, and
determined that using the full input length produced the most relevant output.
The group also determined that more music types needed to be attempted.

From there, we generated dance videos using 12 distinct input clips taken from the kinect
recorded dance seuqences. We also used 6 different pieces of music, including
ambient music with no distinct beat, pop music, classical piano, and a piece
from the AIST dataset. These clips all use a context length of 28, and generates
a full dance of around 10 seconds.

\subsection{MNET}

MNET is a model that uses a GAN architecture to produce future dances. The
pretrained model was not available publicly so we used their public code to
reproduce the training.

\subsubsection{Model Training}

The code had to be modified to fit the training. The hyper-parameters the
researchers used were copied from their paper into the configs for training, and
we used a single A100 GPU to train up to the recommended 500k steps. The output
motion was jittery so further training was ran but the model over-fit the data
heavily after 500k steps.

\subsubsection{In The Wild Generation}
We used MNET to generate videos "in-the-wild" with motion taken from the
recorded sequences from Unity, and music not in the AIST++ dataset. The steps we
took to generate the videos are.

1. using joint2smpl library to convert the 3d joint coordinate data into SMPL
parameters
2. using the SMPL parameters and rendering videos from it to evaluate the motion
quality of the converted motion.
3. after validating the motion resembles the original dance, first 60 seconds of
each clip is used by the model to generate dances with AIST music
4. the validated motion are then used to generate dances with new music

The resulting dance is of poor quality, especially when using new music. The
first output clip is immediately attempts to regress back to the SMPL mean
position with each clip regressing more and more.

This problem is identified to be over fitting, due to the model working well with
training data but not with unseen data. The new model is trained with reduced
parameter count up to 460k steps with one A100 GPU. The resulting model performs
the same

The problem could be the issue with MNET's ability to recognize malformed
dances, since the input motion, although of good enough quality for human, the
exact data could be lower quality and making the model regress to the mean pose.
However the model will not be fit for use if data needs to be well formed, since
input from the kinect will be low quality sometimes, and the model has to be
able to deal with this quality.

\section{Implementation Details}
This section will provide a full overview as well as detailed description into how the implementation is done with an existing interaction framework developed in the Unity Game Engine.

\subsection{Existing Project}
The full body interaction with the AI model is built on top of the full body interaction framework. This section will provide context to what the existing project and the interaction framework achieves as a base to our AI integration.

The existing interaction framework uses various sources of inputs to stream body pose data into the Unity Game Engine. This data is then used to render a Unity Skeleton with 22 joints excluding hand and finger joints. This is important as it maps well with the 24 joint SMPL format.

The skeleton is not displayed directly when the framework is in use, but rather visual effects, such as Ribbons, Sand, etc. The original intent of these effects are to study their impact on human interaction. However, in the context of AI interaction, we want to have the user see the dance of the AI more concretely as to provide a good idea of what the AI is doing.

The input types the framework takes in various in accuracy as well as complexity for setting up, and for use. For example, one source is a full body tracking suit, which is complicated to set up and use, but the positional accuracy of the suit is very high. On the other hand, another source is the Azure Kinect, and while it can be set up in minutes, the accuracy is only good when the user does not cross limbs and faces forward the whole time. We decided to mainly use the Kinect as a source while developing, and testing, since it allowed switching users without additional set up, and the accuracy we get is enough for the AI model.

\subsection{Architecture Overview}
The architecture we have chosen is to run the Unity project and the AI model on two seperate systems. The Unity project which requies drivers for the Azure Kinect needs to be installed on a Windows system, while dependencies for the AI model, including Conda, Python, Cuda drivers, etc., is best suited to run on a Linux system. The way to connect the two system was chosen to simply be HTTP, via REST APIs. The use of a more efficient or streaming system like websockets or gRPC is considered, but as shown after this, the response time of a REST API set up is so minimal it works quite well.

For the AI model to receive data and return the results to the Unity project, we set up a REST endpoint to receive a 2D array of floats as the input payload, as well as parameters specifying the length for generation. This included the input context size, and the music to use. The model runs the input payload and produces the output that is then shipped back to the Unity project as another 2D float array.

The music that is used for inference is sent to the web server in another endpoint where the waveform file binary is directly sent over. The preprocessing is done and saved in memory and can be used by specifying a music index when running inference on the dance input.

On the Unity's side, a game manager records the user's input at 60FPS and ships the input to the web server. The response is used to render a skeleton. Since the response from the server is in 3D joint coordinates, the output can be directly mapped to world space coordinate in the game. However, for displaying poses, we want to render the skeleton mesh, which requires joint rotation rather than coordinate. This rotation was computed using the 3D joint coordinate and calculating the target directional vector of each joint. Then using Unity's Quaternion library's "From To Rotation" to rotate each joint to map it to the exact form as the original output.

The final design of the Unity scene was set up with the user's skeletal mesh and the AI agent's skeletal mesh side by side. The reson for showing the user's own mesh is to show the user their input in terms of the motion capturing. The scene used a wooden floor and a black background to mimic a stage scene, and to provide the user with a more simple and pleasing scene.

\section{}

\end{document}
