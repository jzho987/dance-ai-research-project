\documentclass[final,5p,times,twocolumn,authoryear]{article}
\usepackage{cite}
\begin{document}

\title{Literature Review of Modern Motion and Dance Synthesis Techniques and Their Benefits and Flaws}
\author{Jiazhi Zhou}
\maketitle

\begin{abstract}

This literature review contributes to the research towards better embodiment experience in VR technology. The results of this review will allow the research team to deploy the appropriate model for the given purpose of the experiments and observe the reactions of users to the model.

\end{abstract}

\section{Introduction}

TODO: write

\section{Literature Review}

A literature review is conducted on the domain of human-to-robot
interaction and AI generative dances. The goal of the review is to gain
an initial understanding of the work that is being done in these
domains, as well as providing insights into what could have been done
better, existing models that work well, and how we can model
human-to-robot dance interactions well.

\subsection{Human-AI interaction During Co-creative Process}

Interaction design in Human-Robot co-creative systems is important.
Kantosalo et al. \cite{Kantosalo2020} said that interaction design should be
ground zero for designing co-creative systems. Getting users engaged,
inspired, and actively participating in the co-creative process comes
from good interaction design, however, as \cite{Rezwana2023} pointed
out, many recent research into co-creative AI focus on model performance
but neglect the importance of interaction design. This section of the
review will focus on identifying a suitable framework to adopt for
interaction design, and analysis for our research.

Wallace et al. performed a workshop studying the expectations from
professional dancers for an AI dance partner \cite{Wallace2023}. The workshop introduced to
the dancers on a high level, how a dance generation AI would work. The
dancers are then put into pairs, where one of the dancer from the pair
would explore the role of an AI dancer, and the other one would explore
the role of the user of the AI. The workshop ended with a qualitative
feedback session which resulted in important insights into co-creative
process for dancers. One important theme that a lot of the dancers
mentioned in their feedback was "shared images", more specifically,
having a shared language and understanding of their work, and then,
breaking of the image, allowing them to continue the creative process
exploring other directions, when the current creative direction runs
dry. Another theme was that human dancers would eventually get tired, or
experience physical constraints to their body, but they thought AI
dancers can explore impossible positions which can act as inspiration
rather than a negative. The final theme was that dancers have the
intuition to perdict what their partner will do, resulting in a more
fluid interaction process and allowing them to not look at each other
all the time. This also comes from having a shared image or
understanding of the creative process. These themes should be taken into
consideration for how a dance generation AI should behave from the
perspective of professional dancers, however, this could differ for non-dancers.

Rezwana and Maher proposed a novel framework for designing co-creative
interaction called the COFI. This framework breaks the co-creative
interaction down to low level aspects. This includes: what collaboration style
and communication style does the framework have, is the creative process
a generative process, or a evaluation process or a definition process, how does the AI
contribute to that process, and how much should the AI contribute
compared to the human contribution. The exact framework breaks this down
in a bit more detail, but as an overview, COFI allows us a way to group
different interaction frameworks and compare them to each other. Rezwana
and Maher also mentioned that there are two types of co-creative
processes involving AI, one being a pleasing interaction, and one being
a provoking interaction. A pleasing interaction would have the AI create
what the human wants, while a provoking interaction would have the AI
provide inspiration to the human through "breaking the shared image",
spoken in the language by Wallace et al. For us to align different
terminologies used by different authors, we will discuss Human-AI
interaction using phrases in the COFI framework.

The requirements generated from Wallace's work, can be put into the COFI
framework to solidify interaction desgin when the AI interacts with an
expert dancer. Intuition flow identifies that there would be no distinct
communication between the human and the AI, and the communication style
should all be embodied, and the timing of initiatives should also be
spontaneous. The dancers from Wallace's workshop identified that the
person pretending to be the AI, showed little mimicry, and more
similarity of movement direction and trajectory. This means expert
dancers would likely expect a non-mimicry style. The rest of the
framework is filled in from how a improvisational dance would happen.
Certain points are not very obvious and require further understanding
and analysis. Task distribution could be viewed as both the human and
the AI are dancing. But this can also be viewed as one person is
performing the task of leading, and the other is performing the task of
following, and since in the article, Rezwana and Maher mentioned how
performing the same task often requires turn-taking, this shows us that
we should model the task as being, leader and a follower, and the tasks
are performed in parallel.

% EVERYTHING BELOW ARE UNORGANIZED

The difference how experienced dancers and non-performers communicate
to perform turn-take is studied by Evola et al. \cite{Evola}. 
A improvisational performance is used as the framework for the user
study, where users in a group of 6 can take turns to construct an art
piece from an assortment of items placed on a table in the centre of the
stage. The turn-taking sections of the performances are picked out and analyzed.
One major different identified, between expert performers and the
non-dancers, were the expert performers did not performing any "communicative
movement", but rather using observations from their parafoveal and peripheral
vision to take cues, while the non-performers were seen exchanging gazes to
communicate. This shows that expert dancers have intuition for
turn-taking cues, and that non-dancers will require much more explicit cues
or take longer to initiate a turn-take.

Winston and Magerko explored turn-taking of leadership in a parallel
collaboration style.
\cite{Winston2017}. These cues are explored via implementing a
turn-taking system on the LuminAI framework, to create a new version
that they named, TT-VAI. Winston and Magerko first observed the dancers
performance and through observation, identified that dancers can use
cues like gaze, heptic feedback in the form of applying pressure to the
follower's arm, and energy. The base version of LuminAI and TT-VAI are
studied in a user study. Certain motion cues are used by TT-VAI as a
turn-taking cue, including energy, tempo, and size, since gaze and
heptic are not feasible cues for a AI dancer avatar. Out of the two versions of the
model, the turn taking model was disliked by a few users, who preferred
less "back-and-forth" and preferred more natural interactions and felt
more inspired. 2 users preferred the turn-taking one as it was providing
more than just copying. Mimicry by the agent, however, showed positive
feedback from several participants of the user study, since the agent is
deemed "more responsive to my movement." The research highlighted
interesting ideas in how a sense of leadership state effect's the user's
perception of the model, and how mimicry or being a follower by the
model can provide benefits for improved user perception. Tuning the
agents turn-taking decision making process, and potentially biasing it
towards humans leading could be done to improve the user experience in
future works.

One interesting difference spoted between Winston and Magerko's work, and
Wallace et al. and Evola et al.'s is that Winston and Magerko identified explicit
turn-taking cues while both Wallace and Evola identified the lack of
obvious cues displayed when dancing. This difference can be from the
forms of dancing that they studied, since Winston and Magerko studied
""" and Wallace et al. and Evola et al. both studied improvisational
dance. This could mean that either turn-taking is a way more natural
process in improvisational dance, and there are no clear leader or
follower, or that the dancers do not have a clear idea of who is leading
and who is following, but simply "do", quoting from a participant of
Wallace et al's workshop. If the turn-taking aspect of improvisational
dancing is less, then an effective communication framework could either
be impossible or not useful, and rather focusing on building a shared
image is more important. However, one form of turn-taking discussed
by Wallace is the breaking of shared images. This is the process of
switching the direction of the creative work and starting in a new
direction. Participants idenfied this to being an important expectation
they have from their dance partner. So maybe rather than taking turns,
the AI need to identify when the user is loosing inspiration and provoke
them back into a leading state.

... performed a user study to analyze the effectiveness of two-way
communication in human-AI co-creative system. The system they studied
was called: "Creative Penpal", which is a co-creative drawing program,
where the AI can generate refined and inspiring sketches from user's
line sketch. The user study learned that users prefer the two-way
communication system, where the AI would use text instructions to
communicate with the user, and provide them with options during their
creative process to see inspiring sketches, provide feedback etc. The
qualitative insights gathered mentioned how the users felt more engaged
when the AI provided feedback, and felt more like working with a partner
which they enjoyed. Another theme that was important was that users
identified they enjoyed having control over the AI's input.

Two-way communication in embodied interaction, however, does not mean
explicit vocal commands, and in the case of the game study from Evola et
al.'s work \cite{Evola}, there are not even explicit communication
movements done, by the experts, but they were able to be in sync and
take turns appropriately. The ability to communicate without
communicating can be a display of the "intuition flow" identified by
Wallace et al.'s work.

Both Wallace et al. and Evola et al. identified the importance of having
intuition and a "shared image" when trying to perform seemlessly.
Wallace identified that dancers often predict what their partner will
do, and Evola showed that expert dancers can use context to gain
understanding of turn-taking cues, while non-dancers need explicit gaze
exchange as a way to communicate. 


CSI is a ...

... proposed a way to quickly and effectively label interactions using
three curves:...
The main problem that this framework experienced is the resolution of
the curves produced, as shown in their graph, the alignment of the
curves only aligned on average per minute to 2 minutes, and any
timeframe studied less than a minute has significant variance. This,
although could be used as a reward system for machine learning
algorithms, might not be good for a high quality analysis. However, we
can still study the effectiveness of implementing this framework to
automatically encode interaction data in real-time and design a reward
algorithm to make the system perform better in the interaction.

A two-way communication system should be explored in our interaction
system, although, the communication would be different since embodied
interactions naturally have a communication aspect.

Turn-taking in speech is a good topic to study and some important
aspects of turn-taking could be transferred from speech turn-taking to
dance turn-taking. 
Thomaz and Chao conducted an experiment to learn about
turn-taking in human-robot interaction \cite{Thomaz2011}. The user study is setup by having a human and a
"robot" play a game of simon says, the robot, being teleoperated by a human. The researchers judged each
potential turn-ending indicator against the time it took for the human
to react to the sentence and start their action. All indicators that had
a "negative" reaction time were deemed impossible. This
resulted in the concept of Minimum Necessary Information (MNI) as the
primary indicator in this setting, as humans can start doing an action
after all the necessary words of the sentence are said and the rest is
redundant. This however is not in an improvisational setting, as all
actions and all cues are pre-set, and it is easy to predict the
following words. 

Skantze did a analysis for conversational turn-taking, which is a under
a more improvisational setting \cite{Skantze2021}. The concept of prediction and reaction
are further explored. Prediction was said to play an important factor in
human to human speech interactions, which is what makes it so smooth,
but providing turn-yielding cues are also an important step in the
turn-taking process. The turn-yielding cues are a way for the leader to
yield their current turn to the follower, and the effects of using the
cue is said to be additive, so more turn-yielding cues result in a
better understanding by the follower. But prediction is also an
important factor, just like it was discovered in \cite{Thomaz2011}. The
prediction for the experiment ran by Thomaz and Chao are the MNI point,
where the human can predict the rest of the words, as the phrases are
pre-set.

The concept of breaking shared images from Wallace et al.'s work, can
fit into the turn-taking paradigm where breaking of shared image is a
way that the partner has used to take the leadership and shift the
direction of the dance.

The theme of AI bringing more to the table is explored in both Wallace
et al., and Winston and Magerko's work \cite{Wallace2023Embody,
Winston2017}. The dance professionals from \cite{Wallace2023Embody}
expressed that they would expect their dance partner to be able to shift
the tone of the improvisation to generate more inspiration and keep the
improvisation going. This means bringing something different, like doing
the opposite of what they are doing. Similarly, \cite{Winston2017},
although only 2 of the users expressed this, it is still a point made,
during interactions with TT-VAI, that, they liked it when the AI was
able to do more than just mimic. And although mimicry
had a positive reaction by the non-dancers from \cite{Winston2017}, it
was observed from the experiment in \cite{Wallace2023Embody}, that the
dance professionals rarely mimicked their partner but sometimes copy
things like the trajectory of movement or mirroring their use of space.
This means something that the public could be into, might be less
inspiring for professional dancers. This could mean that even with a
user led interaction, the AI can take the role of the leader when the
user seem disengaged and running out of ideas, then taking the dance in
a different direction to potentially generate some more interest and inspiration.

Another new concept for turn-taking that is particular to dance, is the
perception of leadership state. As in speech, it is obvious who is
leading and who is following, but in fluid interactions like dancing,
the leadership state has to be inferred by both parties. This could
result in a conflict of state, where multiple leaders exist or
no-leaders exist at once. The work by Winston and Magerko only
considered the AI agent's perception of who the leader and follower is,
and did not take into account the user's perception. This should also be
explored further as a way to improve the user's experience.

The turn-taking model of expert dancers can potentially be modeled to
allow a co-creative dance agent to understand turn-taking cues and
perform seemlessly with dancer professionals. The turn-taking for
non-dancers, however, is more complicated, as chances for the
non-dancers to learn the cues and be able to integrate that into their
mental model is likely impossible. However, as Winston and Magerko
suggested, casual users are likely more into user led interactions, or
at least biased towards that. The difference of expert dancers and
casual users interacting with a turn-taking model like TT-VAI should be
investigated to gain insights into if turn-taking should be considered
at all for a dance robot or should it be full or biased towards user led interaction.

\subsection{Dance AI and deep learning}

Machine learning and deep learning has been a popular topic in recent
years. Showing incredible results for text generation, image generation
and much more.  Using deep learning techniques to train machine learning
models could enable the generation of realistic responses to user's full
body inputs in a VR or public installation setting.  This can not only
prompt the users to explore different movements in order to have a
better embodied experience, not also prompt user interactions with the
installation, or become a potent tool for co-creative purposes
\cite{Wallace2023}.  Different techniques and models will be reviewed
and examined on their abilities to generate realistic, diverse and
real-time dances for the purpose of an interactive AI agent.

Alemi et al. explored the idea of an interactive AI agent
\cite{Alemi2017}. Alemi et al. compared the Factored Conditional
Restricted Boltzman Machine (FCRBM) and a Long Short Term Memory (LSTM)
network. And at the time, there was not a large public annotated dance
dataset available like AIST++ \cite{Li2021}, so Alemi et al. had to
record their own dance data which only consisted of 4 dance performances
and a total of 23 minutes of dance and audio data.

Bailando++ is neural netowrk model that generates dances based off of
the previously generated dance sequences \cite{Siyao2023}. The
Bailando++ model is a VQVAE and a Generative Pre-trained Transformer
(GPT) that generates dances from a previous dance sequence and dances in sync
to the music. The model is trained to dance to the music through
the "Actor Critic" learning stage which leverages
reinforcement learning and uses beat-alignment as part of the reward function. This
model was able to achieve top-of-the-line results in motion quality, as
well as motion diversity compared to other
popular dance AI models at the time, including DanceNet,
DanceRevolution, FACT and Li et al. Bailando++ also preformed well in
the user study where users are shown 60 pairs of dances by different
models and voted on "which one is dancing better to the music", where it
was able to achieve at least 88\% win rate against all of the models.
Although Bailando++ focused heavily on the ability to dance to the
music, it is likely that for the purpose of our experiments, the ability
to dance to the music is not as important, as care more about the
ability to prompt interaction. But the technique of using actor-critic
learning can be used in our own model.

Dance with you (DanY) \cite{Yao2023} is a neural network model that generates
dances for a partner dancer for a lead dancer. The model uses a three
stage network that also leverages a VQVAE for encoding and decoding, and
U-Net Models. VQVAE is an auto encoder network which can turn complicated dance data sequences into quantized
codes from a finite code book that is learned through the training of
the VQVAE, and the U-Net takes noised data and turns them into dances
features in the code book, which turns random gaussian noise into
realistic dances. The difference of the DanY model is that not only does
it generate from the condition of audio data like many other models, but
it also generates based on the condition of the lead dancer's dance
sequence. This is important for us since we want to deploy an
interactive AI agent which dances in accordance to the lead dancer, who,
in this case, is the user. The quantitative results from this 
Their proposed AIST-M dataset is a dance dataset that contains
Lead-Partner dancer pair annotation great for training models to
generate partner dances from a lead dance sequence. The techniques they used followed that of the creation of the
AIST++ dataset \cite{Li2021} including tracking, SMPL mesh fitting, and
optimization for filtering out undesirable frames to ensure the quality
of the dance data. The proposed AIST-M dataset will be incredibly useful
for our own models' training and analysis.

\bibliographystyle{plain} 
\bibliography{mybib}

\end{document}
\endinput
